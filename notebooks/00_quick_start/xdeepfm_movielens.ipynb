{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xDeepFM : the eXtreme Deep Factorization Machine \n",
    "This notebook will give you a quick example of how to train an xDeepFM model. \n",
    "xDeepFM \\[1\\] is a deep learning-based model aims at capturing both lower- and higher-order feature interactions for precise recommender systems. Thus it can learn feature interactions more effectively and manual feature engineering effort can be substantially reduced. To summarize, xDeepFM has the following key properties:\n",
    "* It contains a component, named CIN, that learns feature interactions in an explicit fashion and in vector-wise level;\n",
    "* It contains a traditional DNN component that learns feature interactions in an implicit fashion and in bit-wise level.\n",
    "* The implementation makes this model quite configurable. We can enable different subsets of components by setting hyperparameters like `use_Linear_part`, `use_FM_part`, `use_CIN_part`, and `use_DNN_part`. For example, by enabling only the `use_Linear_part` and `use_FM_part`, we can get a classical FM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.5.5 |Anaconda custom (64-bit)| (default, May 13 2018, 21:12:35) \n",
      "[GCC 7.2.0]\n",
      "Tensorflow version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import papermill as pm\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import *\n",
    "from reco_utils.recommender.deeprec.models.xDeepFM import *\n",
    "from reco_utils.recommender.deeprec.IO.iterator import *\n",
    "from reco_utils.dataset.pandas_df_utils import LibffmConverter\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.common.constants import (\n",
    "    DEFAULT_USER_COL as USER_COL,\n",
    "    DEFAULT_ITEM_COL as ITEM_COL,\n",
    "    DEFAULT_RATING_COL as RATING_COL,\n",
    "    DEFAULT_PREDICTION_COL as PREDICT_COL,\n",
    ")\n",
    "from reco_utils.common.constants import SEED\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "xDeepFM uses the FFM format as data input: <label> <field_id>:<feature_id>:<feature_value>\n",
    "\n",
    "    \n",
    "### 1.1 Load Movie rating and genres data\n",
    "First, download MovieLens data. Movies in the data set are tagged as one or more genres where there are total 19 genres including 'unknown'. We load movie genres to use them as item features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.93MB [00:00, 18.5MB/s]                           \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>Genres_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating Genres_string\n",
       "0     196     242     3.0        Comedy\n",
       "1      63     242     3.0        Comedy\n",
       "2     226     242     5.0        Comedy\n",
       "3     154     242     3.0        Comedy\n",
       "4     306     242     5.0        Comedy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MOVIELENS_DATA_SIZE='100k'\n",
    "RANDOM_SEED = SEED  # Set seed for deterministic result\n",
    "# The genres of each movie are returned as '|' separated string, e.g. \"Animation|Children's|Comedy\".\n",
    "data = movielens.load_pandas_df(\n",
    "        size=MOVIELENS_DATA_SIZE,\n",
    "        header=[USER_COL, ITEM_COL, RATING_COL],\n",
    "        genres_col='Genres_string'  # load genres as a temporal column 'Genres_string'\n",
    "    )\n",
    "display(data.head())\n",
    "data['userID'] = data.userID.astype(str)\n",
    "data['itemID'] = data.itemID.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert to libffm format. \n",
    "The output features are  <field_index>:<field_feature_index>:1 or <field_index>:<field_index>:<field_feature_value>, depending on the data type of the features in the original dataframe. Current libffm converter does not support multiple features in one field, so the followings cells are a workaround \n",
    "\n",
    "There are 3 fields in this dataset: userID, itemID and genres.\n",
    "For fields of 'userID' and 'itemID', they are fields with single feature. But for the field 'genres', it has 19 features which indicates different types of genre.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres: ['Action' 'Adventure' 'Animation' \"Children's\" 'Comedy' 'Crime'\n",
      " 'Documentary' 'Drama' 'Fantasy' 'Film-Noir' 'Horror' 'Musical' 'Mystery'\n",
      " 'Romance' 'Sci-Fi' 'Thriller' 'War' 'Western' 'unknown']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>Genres_string</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>302</td>\n",
       "      <td>Crime|Film-Noir|Mystery|Thriller</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>377</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>51</td>\n",
       "      <td>Drama|Romance|War|Western</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>346</td>\n",
       "      <td>Crime|Drama</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    itemID                     Genres_string  \\\n",
       "0      242                            Comedy   \n",
       "117    302  Crime|Film-Noir|Mystery|Thriller   \n",
       "414    377                 Children's|Comedy   \n",
       "427     51         Drama|Romance|War|Western   \n",
       "508    346                       Crime|Drama   \n",
       "\n",
       "                                                genres  \n",
       "0    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "117  [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, ...  \n",
       "414  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "427  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "508  [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "data['genres'] = genres_encoder.fit_transform(\n",
    "        data['Genres_string'].apply(lambda s: s.split(\"|\"))\n",
    "    ).tolist()\n",
    "print(\"Genres:\", genres_encoder.classes_)\n",
    "display(data.drop_duplicates(ITEM_COL)[[ITEM_COL, 'Genres_string', 'genres']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate new column names for field 'genre'\n",
    "data[['genre1','genre2','genre3','genre4','genre5','genre6','genre7',\\\n",
    "      'genre8','genre9','genre10','genre11','genre12','genre13','genre14',\\\n",
    "      'genre15','genre16','genre17','genre18','genre19']] \\\n",
    "=pd.DataFrame(data.genres.values.tolist(), index= data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\"genres\",\"Genres_string\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>genre4</th>\n",
       "      <th>genre5</th>\n",
       "      <th>genre6</th>\n",
       "      <th>genre7</th>\n",
       "      <th>...</th>\n",
       "      <th>genre10</th>\n",
       "      <th>genre11</th>\n",
       "      <th>genre12</th>\n",
       "      <th>genre13</th>\n",
       "      <th>genre14</th>\n",
       "      <th>genre15</th>\n",
       "      <th>genre16</th>\n",
       "      <th>genre17</th>\n",
       "      <th>genre18</th>\n",
       "      <th>genre19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  userID itemID  rating  genre1  genre2  genre3  genre4  genre5  genre6  \\\n",
       "0    196    242     3.0       0       0       0       0       1       0   \n",
       "1     63    242     3.0       0       0       0       0       1       0   \n",
       "2    226    242     5.0       0       0       0       0       1       0   \n",
       "3    154    242     3.0       0       0       0       0       1       0   \n",
       "4    306    242     5.0       0       0       0       0       1       0   \n",
       "\n",
       "   genre7   ...     genre10  genre11  genre12  genre13  genre14  genre15  \\\n",
       "0       0   ...           0        0        0        0        0        0   \n",
       "1       0   ...           0        0        0        0        0        0   \n",
       "2       0   ...           0        0        0        0        0        0   \n",
       "3       0   ...           0        0        0        0        0        0   \n",
       "4       0   ...           0        0        0        0        0        0   \n",
       "\n",
       "   genre16  genre17  genre18  genre19  \n",
       "0        0        0        0        0  \n",
       "1        0        0        0        0  \n",
       "2        0        0        0        0  \n",
       "3        0        0        0        0  \n",
       "4        0        0        0        0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feed the data into libffm converter. \n",
    "converter = LibffmConverter().fit(data, col_rating=RATING_COL)\n",
    "data_transformed = converter.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>genre4</th>\n",
       "      <th>genre5</th>\n",
       "      <th>genre6</th>\n",
       "      <th>genre7</th>\n",
       "      <th>...</th>\n",
       "      <th>genre10</th>\n",
       "      <th>genre11</th>\n",
       "      <th>genre12</th>\n",
       "      <th>genre13</th>\n",
       "      <th>genre14</th>\n",
       "      <th>genre15</th>\n",
       "      <th>genre16</th>\n",
       "      <th>genre17</th>\n",
       "      <th>genre18</th>\n",
       "      <th>genre19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:1:1</td>\n",
       "      <td>2:944:1</td>\n",
       "      <td>3:2626:0</td>\n",
       "      <td>4:2627:0</td>\n",
       "      <td>5:2628:0</td>\n",
       "      <td>6:2629:0</td>\n",
       "      <td>7:2630:1</td>\n",
       "      <td>8:2631:0</td>\n",
       "      <td>9:2632:0</td>\n",
       "      <td>...</td>\n",
       "      <td>12:2635:0</td>\n",
       "      <td>13:2636:0</td>\n",
       "      <td>14:2637:0</td>\n",
       "      <td>15:2638:0</td>\n",
       "      <td>16:2639:0</td>\n",
       "      <td>17:2640:0</td>\n",
       "      <td>18:2641:0</td>\n",
       "      <td>19:2642:0</td>\n",
       "      <td>20:2643:0</td>\n",
       "      <td>21:2644:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:2:1</td>\n",
       "      <td>2:944:1</td>\n",
       "      <td>3:2626:0</td>\n",
       "      <td>4:2627:0</td>\n",
       "      <td>5:2628:0</td>\n",
       "      <td>6:2629:0</td>\n",
       "      <td>7:2630:1</td>\n",
       "      <td>8:2631:0</td>\n",
       "      <td>9:2632:0</td>\n",
       "      <td>...</td>\n",
       "      <td>12:2635:0</td>\n",
       "      <td>13:2636:0</td>\n",
       "      <td>14:2637:0</td>\n",
       "      <td>15:2638:0</td>\n",
       "      <td>16:2639:0</td>\n",
       "      <td>17:2640:0</td>\n",
       "      <td>18:2641:0</td>\n",
       "      <td>19:2642:0</td>\n",
       "      <td>20:2643:0</td>\n",
       "      <td>21:2644:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1:3:1</td>\n",
       "      <td>2:944:1</td>\n",
       "      <td>3:2626:0</td>\n",
       "      <td>4:2627:0</td>\n",
       "      <td>5:2628:0</td>\n",
       "      <td>6:2629:0</td>\n",
       "      <td>7:2630:1</td>\n",
       "      <td>8:2631:0</td>\n",
       "      <td>9:2632:0</td>\n",
       "      <td>...</td>\n",
       "      <td>12:2635:0</td>\n",
       "      <td>13:2636:0</td>\n",
       "      <td>14:2637:0</td>\n",
       "      <td>15:2638:0</td>\n",
       "      <td>16:2639:0</td>\n",
       "      <td>17:2640:0</td>\n",
       "      <td>18:2641:0</td>\n",
       "      <td>19:2642:0</td>\n",
       "      <td>20:2643:0</td>\n",
       "      <td>21:2644:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:4:1</td>\n",
       "      <td>2:944:1</td>\n",
       "      <td>3:2626:0</td>\n",
       "      <td>4:2627:0</td>\n",
       "      <td>5:2628:0</td>\n",
       "      <td>6:2629:0</td>\n",
       "      <td>7:2630:1</td>\n",
       "      <td>8:2631:0</td>\n",
       "      <td>9:2632:0</td>\n",
       "      <td>...</td>\n",
       "      <td>12:2635:0</td>\n",
       "      <td>13:2636:0</td>\n",
       "      <td>14:2637:0</td>\n",
       "      <td>15:2638:0</td>\n",
       "      <td>16:2639:0</td>\n",
       "      <td>17:2640:0</td>\n",
       "      <td>18:2641:0</td>\n",
       "      <td>19:2642:0</td>\n",
       "      <td>20:2643:0</td>\n",
       "      <td>21:2644:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1:5:1</td>\n",
       "      <td>2:944:1</td>\n",
       "      <td>3:2626:0</td>\n",
       "      <td>4:2627:0</td>\n",
       "      <td>5:2628:0</td>\n",
       "      <td>6:2629:0</td>\n",
       "      <td>7:2630:1</td>\n",
       "      <td>8:2631:0</td>\n",
       "      <td>9:2632:0</td>\n",
       "      <td>...</td>\n",
       "      <td>12:2635:0</td>\n",
       "      <td>13:2636:0</td>\n",
       "      <td>14:2637:0</td>\n",
       "      <td>15:2638:0</td>\n",
       "      <td>16:2639:0</td>\n",
       "      <td>17:2640:0</td>\n",
       "      <td>18:2641:0</td>\n",
       "      <td>19:2642:0</td>\n",
       "      <td>20:2643:0</td>\n",
       "      <td>21:2644:0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating userID   itemID    genre1    genre2    genre3    genre4    genre5  \\\n",
       "0     3.0  1:1:1  2:944:1  3:2626:0  4:2627:0  5:2628:0  6:2629:0  7:2630:1   \n",
       "1     3.0  1:2:1  2:944:1  3:2626:0  4:2627:0  5:2628:0  6:2629:0  7:2630:1   \n",
       "2     5.0  1:3:1  2:944:1  3:2626:0  4:2627:0  5:2628:0  6:2629:0  7:2630:1   \n",
       "3     3.0  1:4:1  2:944:1  3:2626:0  4:2627:0  5:2628:0  6:2629:0  7:2630:1   \n",
       "4     5.0  1:5:1  2:944:1  3:2626:0  4:2627:0  5:2628:0  6:2629:0  7:2630:1   \n",
       "\n",
       "     genre6    genre7    ...        genre10    genre11    genre12    genre13  \\\n",
       "0  8:2631:0  9:2632:0    ...      12:2635:0  13:2636:0  14:2637:0  15:2638:0   \n",
       "1  8:2631:0  9:2632:0    ...      12:2635:0  13:2636:0  14:2637:0  15:2638:0   \n",
       "2  8:2631:0  9:2632:0    ...      12:2635:0  13:2636:0  14:2637:0  15:2638:0   \n",
       "3  8:2631:0  9:2632:0    ...      12:2635:0  13:2636:0  14:2637:0  15:2638:0   \n",
       "4  8:2631:0  9:2632:0    ...      12:2635:0  13:2636:0  14:2637:0  15:2638:0   \n",
       "\n",
       "     genre14    genre15    genre16    genre17    genre18    genre19  \n",
       "0  16:2639:0  17:2640:0  18:2641:0  19:2642:0  20:2643:0  21:2644:0  \n",
       "1  16:2639:0  17:2640:0  18:2641:0  19:2642:0  20:2643:0  21:2644:0  \n",
       "2  16:2639:0  17:2640:0  18:2641:0  19:2642:0  20:2643:0  21:2644:0  \n",
       "3  16:2639:0  17:2640:0  18:2641:0  19:2642:0  20:2643:0  21:2644:0  \n",
       "4  16:2639:0  17:2640:0  18:2641:0  19:2642:0  20:2643:0  21:2644:0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is a workaround for current libffmconverter\n",
    "## keep field number for genre to 3 and reset genre index to 1:19\n",
    "num = int(data_transformed['genre1'][0][2:-2])\n",
    "\n",
    "genre_index=1\n",
    "old_field_index=genre_index+2\n",
    "\n",
    "for i in range(num,num+19):\n",
    "    \n",
    "    old_index=str(old_field_index)+':'+str(i)\n",
    "    new_index='3:'+str(genre_index)\n",
    "    \n",
    "    coln='genre'+str(genre_index)\n",
    "    data_transformed[coln]=data_transformed[coln].str.replace(old_index,new_index)\n",
    "    \n",
    "    \n",
    "    genre_index=genre_index+1\n",
    "    old_field_index=old_field_index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "      <th>genre4</th>\n",
       "      <th>genre5</th>\n",
       "      <th>genre6</th>\n",
       "      <th>genre7</th>\n",
       "      <th>...</th>\n",
       "      <th>genre10</th>\n",
       "      <th>genre11</th>\n",
       "      <th>genre12</th>\n",
       "      <th>genre13</th>\n",
       "      <th>genre14</th>\n",
       "      <th>genre15</th>\n",
       "      <th>genre16</th>\n",
       "      <th>genre17</th>\n",
       "      <th>genre18</th>\n",
       "      <th>genre19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1:505:1</td>\n",
       "      <td>2:2621:1</td>\n",
       "      <td>3:1:0</td>\n",
       "      <td>3:2:0</td>\n",
       "      <td>3:3:0</td>\n",
       "      <td>3:4:0</td>\n",
       "      <td>3:5:0</td>\n",
       "      <td>3:6:0</td>\n",
       "      <td>3:7:0</td>\n",
       "      <td>...</td>\n",
       "      <td>3:10:0</td>\n",
       "      <td>3:11:0</td>\n",
       "      <td>3:12:0</td>\n",
       "      <td>3:13:0</td>\n",
       "      <td>3:14:0</td>\n",
       "      <td>3:15:0</td>\n",
       "      <td>3:16:0</td>\n",
       "      <td>3:17:0</td>\n",
       "      <td>3:18:0</td>\n",
       "      <td>3:19:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:89:1</td>\n",
       "      <td>2:2622:1</td>\n",
       "      <td>3:1:0</td>\n",
       "      <td>3:2:0</td>\n",
       "      <td>3:3:0</td>\n",
       "      <td>3:4:0</td>\n",
       "      <td>3:5:0</td>\n",
       "      <td>3:6:0</td>\n",
       "      <td>3:7:0</td>\n",
       "      <td>...</td>\n",
       "      <td>3:10:0</td>\n",
       "      <td>3:11:0</td>\n",
       "      <td>3:12:0</td>\n",
       "      <td>3:13:0</td>\n",
       "      <td>3:14:0</td>\n",
       "      <td>3:15:0</td>\n",
       "      <td>3:16:0</td>\n",
       "      <td>3:17:0</td>\n",
       "      <td>3:18:0</td>\n",
       "      <td>3:19:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:89:1</td>\n",
       "      <td>2:2623:1</td>\n",
       "      <td>3:1:0</td>\n",
       "      <td>3:2:0</td>\n",
       "      <td>3:3:0</td>\n",
       "      <td>3:4:0</td>\n",
       "      <td>3:5:0</td>\n",
       "      <td>3:6:0</td>\n",
       "      <td>3:7:0</td>\n",
       "      <td>...</td>\n",
       "      <td>3:10:0</td>\n",
       "      <td>3:11:0</td>\n",
       "      <td>3:12:0</td>\n",
       "      <td>3:13:0</td>\n",
       "      <td>3:14:0</td>\n",
       "      <td>3:15:0</td>\n",
       "      <td>3:16:0</td>\n",
       "      <td>3:17:0</td>\n",
       "      <td>3:18:0</td>\n",
       "      <td>3:19:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:89:1</td>\n",
       "      <td>2:2624:1</td>\n",
       "      <td>3:1:0</td>\n",
       "      <td>3:2:0</td>\n",
       "      <td>3:3:0</td>\n",
       "      <td>3:4:0</td>\n",
       "      <td>3:5:0</td>\n",
       "      <td>3:6:0</td>\n",
       "      <td>3:7:0</td>\n",
       "      <td>...</td>\n",
       "      <td>3:10:0</td>\n",
       "      <td>3:11:0</td>\n",
       "      <td>3:12:0</td>\n",
       "      <td>3:13:0</td>\n",
       "      <td>3:14:0</td>\n",
       "      <td>3:15:0</td>\n",
       "      <td>3:16:0</td>\n",
       "      <td>3:17:0</td>\n",
       "      <td>3:18:0</td>\n",
       "      <td>3:19:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1:89:1</td>\n",
       "      <td>2:2625:1</td>\n",
       "      <td>3:1:0</td>\n",
       "      <td>3:2:0</td>\n",
       "      <td>3:3:0</td>\n",
       "      <td>3:4:0</td>\n",
       "      <td>3:5:0</td>\n",
       "      <td>3:6:0</td>\n",
       "      <td>3:7:1</td>\n",
       "      <td>...</td>\n",
       "      <td>3:10:0</td>\n",
       "      <td>3:11:0</td>\n",
       "      <td>3:12:0</td>\n",
       "      <td>3:13:0</td>\n",
       "      <td>3:14:0</td>\n",
       "      <td>3:15:0</td>\n",
       "      <td>3:16:0</td>\n",
       "      <td>3:17:0</td>\n",
       "      <td>3:18:0</td>\n",
       "      <td>3:19:0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating   userID    itemID genre1 genre2 genre3 genre4 genre5 genre6  \\\n",
       "99995     4.0  1:505:1  2:2621:1  3:1:0  3:2:0  3:3:0  3:4:0  3:5:0  3:6:0   \n",
       "99996     3.0   1:89:1  2:2622:1  3:1:0  3:2:0  3:3:0  3:4:0  3:5:0  3:6:0   \n",
       "99997     3.0   1:89:1  2:2623:1  3:1:0  3:2:0  3:3:0  3:4:0  3:5:0  3:6:0   \n",
       "99998     3.0   1:89:1  2:2624:1  3:1:0  3:2:0  3:3:0  3:4:0  3:5:0  3:6:0   \n",
       "99999     3.0   1:89:1  2:2625:1  3:1:0  3:2:0  3:3:0  3:4:0  3:5:0  3:6:0   \n",
       "\n",
       "      genre7   ...   genre10 genre11 genre12 genre13 genre14 genre15 genre16  \\\n",
       "99995  3:7:0   ...    3:10:0  3:11:0  3:12:0  3:13:0  3:14:0  3:15:0  3:16:0   \n",
       "99996  3:7:0   ...    3:10:0  3:11:0  3:12:0  3:13:0  3:14:0  3:15:0  3:16:0   \n",
       "99997  3:7:0   ...    3:10:0  3:11:0  3:12:0  3:13:0  3:14:0  3:15:0  3:16:0   \n",
       "99998  3:7:0   ...    3:10:0  3:11:0  3:12:0  3:13:0  3:14:0  3:15:0  3:16:0   \n",
       "99999  3:7:1   ...    3:10:0  3:11:0  3:12:0  3:13:0  3:14:0  3:15:0  3:16:0   \n",
       "\n",
       "      genre17 genre18 genre19  \n",
       "99995  3:17:0  3:18:0  3:19:0  \n",
       "99996  3:17:0  3:18:0  3:19:0  \n",
       "99997  3:17:0  3:18:0  3:19:0  \n",
       "99998  3:17:0  3:18:0  3:19:0  \n",
       "99999  3:17:0  3:18:0  3:19:0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "xDeepFM uses the FFM format as data input: `<label> <field_id>:<feature_id>:<feature_value>`  \n",
    "Each line represents an instance, `<label>` is a binary value with 1 meaning positive instance and 0 meaning negative instance. \n",
    "Features are divided into fields. For example, user's gender is a field, it contains three possible values, i.e. male, female and unknown. Occupation can be another field, which contains many more possible values than the gender field. Both field index and feature index are starting from 1. <br>\n",
    "Now let's start with movielens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../tests/resources/deeprec/movielens'\n",
    "yaml_file = os.path.join(data_path, r'network_xdeepFM.yaml')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "train_file= os.path.join(data_path,'train.csv')\n",
    "test_file=os.path.join(data_path,'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_random_split(\n",
    "        data_transformed,\n",
    "        ratio=0.75,\n",
    "        seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(train_file, sep=' ',header=False, index=False)\n",
    "test.to_csv(test_file, sep=' ',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters\n",
    "prepare_hparams() will create a full set of hyper-parameters for model training, such as learning rate, feature number, and dropout ratio. We can put those parameters in a yaml file, or pass parameters as the function's parameters (which will overwrite yaml settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DNN_FIELD_NUM', None), ('FEATURE_COUNT', 22), ('FIELD_COUNT', 3), ('MODEL_DIR', None), ('PAIR_NUM', None), ('SUMMARIES_DIR', None), ('activation', ['relu', 'relu']), ('attention_activation', None), ('attention_dropout', 0.0), ('attention_layer_sizes', None), ('batch_size', 128), ('cross_activation', 'identity'), ('cross_l1', 0.0), ('cross_l2', 0.0), ('cross_layer_sizes', [100, 100, 50]), ('cross_layers', None), ('data_format', 'ffm'), ('dim', 10), ('doc_size', None), ('dropout', [0.0, 0.0]), ('dtype', 32), ('embed_l1', 0.0), ('embed_l2', 0.0), ('enable_BN', False), ('entityEmb_file', None), ('entity_dim', None), ('entity_embedding_method', None), ('entity_size', None), ('epochs', 10), ('fast_CIN_d', 0), ('filter_sizes', None), ('init_method', 'tnormal'), ('init_value', 0.01), ('is_clip_norm', 0), ('iterator_type', None), ('kg_file', None), ('kg_training_interval', 5), ('layer_l1', 0.0), ('layer_l2', 0.0), ('layer_sizes', [400, 400]), ('learning_rate', 0.001), ('load_model_name', None), ('load_saved_model', False), ('loss', 'square_loss'), ('lr_kg', 0.5), ('lr_rs', 1), ('max_grad_norm', 2), ('method', 'regression'), ('metrics', ['rmse', 'mae', 'rsquare', 'exp_var']), ('model_type', 'xDeepFM'), ('mu', None), ('n_item', None), ('n_item_attr', None), ('n_user', None), ('n_user_attr', None), ('num_filters', None), ('optimizer', 'adam'), ('ranking_metrics', ['map_at_k', 'ndcg_at_k', 'precision_at_k', 'recall_at_k']), ('reg_kg', 0.0), ('save_epoch', 2), ('save_model', False), ('show_step', 20), ('top_K', 10), ('train_ratio', None), ('transform', None), ('use_CIN_part', True), ('use_DNN_part', False), ('use_FM_part', False), ('use_Linear_part', False), ('user_clicks', None), ('user_dropout', False), ('user_item_file', '../../tests/resources/deeprec/movielens/user_item.csv'), ('wordEmb_file', None), ('word_size', None), ('write_tfevents', False)]\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file) ##\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader\n",
    "Designate a data iterator for the model. xDeepFM uses FFMTextIterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = FFMTextIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "When both hyper-parameters and data iterator are ready, we can create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add CIN part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = XDeepFMModel(hparams, input_creator)\n",
    "\n",
    "## sometimes we don't want to train a model from scratch\n",
    "## then we can load a pre-trained model like this: \n",
    "#model.load_model(r'your_model_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and fit model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 3.7089, data_loss: 3.7089\n",
      "step 40 , total_loss: 3.6294, data_loss: 3.6294\n",
      "step 60 , total_loss: 3.8295, data_loss: 3.8295\n",
      "step 80 , total_loss: 3.6042, data_loss: 3.6042\n",
      "step 100 , total_loss: 3.5788, data_loss: 3.5788\n",
      "step 120 , total_loss: 3.5110, data_loss: 3.5110\n",
      "step 140 , total_loss: 3.3969, data_loss: 3.3969\n",
      "step 160 , total_loss: 3.5492, data_loss: 3.5492\n",
      "step 180 , total_loss: 3.5309, data_loss: 3.5309\n",
      "step 200 , total_loss: 3.4175, data_loss: 3.4175\n",
      "step 220 , total_loss: 3.4632, data_loss: 3.4632\n",
      "step 240 , total_loss: 3.4177, data_loss: 3.4177\n",
      "step 260 , total_loss: 3.4410, data_loss: 3.4410\n",
      "step 280 , total_loss: 3.4789, data_loss: 3.4789\n",
      "step 300 , total_loss: 3.2412, data_loss: 3.2412\n",
      "step 320 , total_loss: 3.3942, data_loss: 3.3942\n",
      "step 340 , total_loss: 3.3369, data_loss: 3.3369\n",
      "step 360 , total_loss: 3.2190, data_loss: 3.2190\n",
      "step 380 , total_loss: 3.4219, data_loss: 3.4219\n",
      "step 400 , total_loss: 3.2911, data_loss: 3.2911\n",
      "step 420 , total_loss: 3.2981, data_loss: 3.2981\n",
      "step 440 , total_loss: 3.2702, data_loss: 3.2702\n",
      "step 460 , total_loss: 3.3770, data_loss: 3.3770\n",
      "step 480 , total_loss: 3.1709, data_loss: 3.1709\n",
      "step 500 , total_loss: 3.2495, data_loss: 3.2495\n",
      "step 520 , total_loss: 3.0666, data_loss: 3.0666\n",
      "step 540 , total_loss: 3.1836, data_loss: 3.1836\n",
      "step 560 , total_loss: 3.1833, data_loss: 3.1833\n",
      "step 580 , total_loss: 3.1884, data_loss: 3.1884\n",
      "at epoch 1 train info: exp_var:-0.17458438873291016, mae:2.900178, rmse:3.1237478, rsquare:-6.721108736437676 eval info: exp_var:-0.1755540370941162, mae:2.8886726, rmse:3.114097, rsquare:-6.5938041399087775\n",
      "at epoch 1 , train time: 14.1 eval time: 14.8\n",
      "step 20 , total_loss: 3.1613, data_loss: 3.1613\n",
      "step 40 , total_loss: 3.0447, data_loss: 3.0447\n",
      "step 60 , total_loss: 3.2077, data_loss: 3.2077\n",
      "step 80 , total_loss: 3.0559, data_loss: 3.0559\n",
      "step 100 , total_loss: 3.0356, data_loss: 3.0356\n",
      "step 120 , total_loss: 2.9545, data_loss: 2.9545\n",
      "step 140 , total_loss: 2.8595, data_loss: 2.8595\n",
      "step 160 , total_loss: 2.9944, data_loss: 2.9944\n",
      "step 180 , total_loss: 2.9940, data_loss: 2.9940\n",
      "step 200 , total_loss: 2.8831, data_loss: 2.8831\n",
      "step 220 , total_loss: 2.9387, data_loss: 2.9387\n",
      "step 240 , total_loss: 2.8825, data_loss: 2.8825\n",
      "step 260 , total_loss: 2.9110, data_loss: 2.9110\n",
      "step 280 , total_loss: 2.9372, data_loss: 2.9372\n",
      "step 300 , total_loss: 2.7079, data_loss: 2.7079\n",
      "step 320 , total_loss: 2.8700, data_loss: 2.8700\n",
      "step 340 , total_loss: 2.8047, data_loss: 2.8047\n",
      "step 360 , total_loss: 2.6818, data_loss: 2.6818\n",
      "step 380 , total_loss: 2.8930, data_loss: 2.8930\n",
      "step 400 , total_loss: 2.7509, data_loss: 2.7509\n",
      "step 420 , total_loss: 2.7602, data_loss: 2.7602\n",
      "step 440 , total_loss: 2.7443, data_loss: 2.7443\n",
      "step 460 , total_loss: 2.8430, data_loss: 2.8430\n",
      "step 480 , total_loss: 2.6389, data_loss: 2.6389\n",
      "step 500 , total_loss: 2.7183, data_loss: 2.7183\n",
      "step 520 , total_loss: 2.5456, data_loss: 2.5456\n",
      "step 540 , total_loss: 2.6492, data_loss: 2.6492\n",
      "step 560 , total_loss: 2.6631, data_loss: 2.6631\n",
      "step 580 , total_loss: 2.6653, data_loss: 2.6653\n",
      "at epoch 2 train info: exp_var:-0.12826073169708252, mae:2.357989, rmse:2.6022105, rsquare:-4.358101158840903 eval info: exp_var:-0.12947773933410645, mae:2.3470173, rmse:2.593588, rsquare:-4.267401945517926\n",
      "at epoch 2 , train time: 13.1 eval time: 15.4\n",
      "step 20 , total_loss: 2.6338, data_loss: 2.6338\n",
      "step 40 , total_loss: 2.5284, data_loss: 2.5284\n",
      "step 60 , total_loss: 2.6906, data_loss: 2.6906\n",
      "step 80 , total_loss: 2.5411, data_loss: 2.5411\n",
      "step 100 , total_loss: 2.5193, data_loss: 2.5193\n",
      "step 120 , total_loss: 2.4385, data_loss: 2.4385\n",
      "step 140 , total_loss: 2.3474, data_loss: 2.3474\n",
      "step 160 , total_loss: 2.4797, data_loss: 2.4797\n",
      "step 180 , total_loss: 2.4842, data_loss: 2.4842\n",
      "step 200 , total_loss: 2.3826, data_loss: 2.3826\n",
      "step 220 , total_loss: 2.4173, data_loss: 2.4173\n",
      "step 240 , total_loss: 2.3708, data_loss: 2.3708\n",
      "step 260 , total_loss: 2.4046, data_loss: 2.4046\n",
      "step 280 , total_loss: 2.4201, data_loss: 2.4201\n",
      "step 300 , total_loss: 2.2030, data_loss: 2.2030\n",
      "step 320 , total_loss: 2.3761, data_loss: 2.3761\n",
      "step 340 , total_loss: 2.3019, data_loss: 2.3019\n",
      "step 360 , total_loss: 2.1730, data_loss: 2.1730\n",
      "step 380 , total_loss: 2.3948, data_loss: 2.3948\n",
      "step 400 , total_loss: 2.2359, data_loss: 2.2359\n",
      "step 420 , total_loss: 2.2497, data_loss: 2.2497\n",
      "step 440 , total_loss: 2.2479, data_loss: 2.2479\n",
      "step 460 , total_loss: 2.3377, data_loss: 2.3377\n",
      "step 480 , total_loss: 2.1348, data_loss: 2.1348\n",
      "step 500 , total_loss: 2.2153, data_loss: 2.2153\n",
      "step 520 , total_loss: 2.0593, data_loss: 2.0593\n",
      "step 540 , total_loss: 2.1465, data_loss: 2.1465\n",
      "step 560 , total_loss: 2.1781, data_loss: 2.1781\n",
      "step 580 , total_loss: 2.1725, data_loss: 2.1725\n",
      "at epoch 3 train info: exp_var:-0.07711684703826904, mae:1.8762925, rmse:2.1149232, rsquare:-2.539313742233633 eval info: exp_var:-0.07912349700927734, mae:1.866724, rmse:2.1078663, rsquare:-2.4792153850238607\n",
      "at epoch 3 , train time: 13.3 eval time: 15.3\n",
      "step 20 , total_loss: 2.1418, data_loss: 2.1418\n",
      "step 40 , total_loss: 2.0509, data_loss: 2.0509\n",
      "step 60 , total_loss: 2.2061, data_loss: 2.2061\n",
      "step 80 , total_loss: 2.0678, data_loss: 2.0678\n",
      "step 100 , total_loss: 2.0402, data_loss: 2.0402\n",
      "step 120 , total_loss: 1.9609, data_loss: 1.9609\n",
      "step 140 , total_loss: 1.8802, data_loss: 1.8802\n",
      "step 160 , total_loss: 2.0036, data_loss: 2.0036\n",
      "step 180 , total_loss: 2.0129, data_loss: 2.0129\n",
      "step 200 , total_loss: 1.9277, data_loss: 1.9277\n",
      "step 220 , total_loss: 1.9411, data_loss: 1.9411\n",
      "step 240 , total_loss: 1.9021, data_loss: 1.9021\n",
      "step 260 , total_loss: 1.9442, data_loss: 1.9442\n",
      "step 280 , total_loss: 1.9477, data_loss: 1.9477\n",
      "step 300 , total_loss: 1.7496, data_loss: 1.7496\n",
      "step 320 , total_loss: 1.9310, data_loss: 1.9310\n",
      "step 340 , total_loss: 1.8516, data_loss: 1.8516\n",
      "step 360 , total_loss: 1.7163, data_loss: 1.7163\n",
      "step 380 , total_loss: 1.9496, data_loss: 1.9496\n",
      "step 400 , total_loss: 1.7676, data_loss: 1.7676\n",
      "step 420 , total_loss: 1.7865, data_loss: 1.7865\n",
      "step 440 , total_loss: 1.8049, data_loss: 1.8049\n",
      "step 460 , total_loss: 1.8841, data_loss: 1.8841\n",
      "step 480 , total_loss: 1.6823, data_loss: 1.6823\n",
      "step 500 , total_loss: 1.7634, data_loss: 1.7634\n",
      "step 520 , total_loss: 1.6372, data_loss: 1.6372\n",
      "step 540 , total_loss: 1.7011, data_loss: 1.7011\n",
      "step 560 , total_loss: 1.7545, data_loss: 1.7545\n",
      "step 580 , total_loss: 1.7365, data_loss: 1.7365\n",
      "at epoch 4 train info: exp_var:-0.038622498512268066, mae:1.4730381, rmse:1.6896449, rsquare:-1.2590471515453507 eval info: exp_var:-0.04123425483703613, mae:1.4658825, rmse:1.6850519, rsquare:-1.2234306562514687\n",
      "at epoch 4 , train time: 13.3 eval time: 15.0\n",
      "step 20 , total_loss: 1.7136, data_loss: 1.7136\n",
      "step 40 , total_loss: 1.6426, data_loss: 1.6426\n",
      "step 60 , total_loss: 1.7779, data_loss: 1.7779\n",
      "step 80 , total_loss: 1.6680, data_loss: 1.6680\n",
      "step 100 , total_loss: 1.6296, data_loss: 1.6296\n",
      "step 120 , total_loss: 1.5527, data_loss: 1.5527\n",
      "step 140 , total_loss: 1.4923, data_loss: 1.4923\n",
      "step 160 , total_loss: 1.5969, data_loss: 1.5969\n",
      "step 180 , total_loss: 1.6101, data_loss: 1.6101\n",
      "step 200 , total_loss: 1.5534, data_loss: 1.5534\n",
      "step 220 , total_loss: 1.5435, data_loss: 1.5435\n",
      "step 240 , total_loss: 1.5112, data_loss: 1.5112\n",
      "step 260 , total_loss: 1.5654, data_loss: 1.5654\n",
      "step 280 , total_loss: 1.5553, data_loss: 1.5553\n",
      "step 300 , total_loss: 1.3901, data_loss: 1.3901\n",
      "step 320 , total_loss: 1.5693, data_loss: 1.5693\n",
      "step 340 , total_loss: 1.4934, data_loss: 1.4934\n",
      "step 360 , total_loss: 1.3545, data_loss: 1.3545\n",
      "step 380 , total_loss: 1.5953, data_loss: 1.5953\n",
      "step 400 , total_loss: 1.3849, data_loss: 1.3849\n",
      "step 420 , total_loss: 1.4092, data_loss: 1.4092\n",
      "step 440 , total_loss: 1.4555, data_loss: 1.4555\n",
      "step 460 , total_loss: 1.5209, data_loss: 1.5209\n",
      "step 480 , total_loss: 1.3236, data_loss: 1.3236\n",
      "step 500 , total_loss: 1.4025, data_loss: 1.4025\n",
      "step 520 , total_loss: 1.3262, data_loss: 1.3262\n",
      "step 540 , total_loss: 1.3569, data_loss: 1.3569\n",
      "step 560 , total_loss: 1.4348, data_loss: 1.4348\n",
      "step 580 , total_loss: 1.4005, data_loss: 1.4005\n",
      "at epoch 5 train info: exp_var:-0.011698007583618164, mae:1.166059, rmse:1.3705838, rsquare:-0.48641608689214033 eval info: exp_var:-0.01480245590209961, mae:1.1620718, rmse:1.369489, rsquare:-0.46861294145072163\n",
      "at epoch 5 , train time: 13.4 eval time: 15.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20 , total_loss: 1.3941, data_loss: 1.3941\n",
      "step 40 , total_loss: 1.3499, data_loss: 1.3499\n",
      "step 60 , total_loss: 1.4432, data_loss: 1.4432\n",
      "step 80 , total_loss: 1.3876, data_loss: 1.3876\n",
      "step 100 , total_loss: 1.3355, data_loss: 1.3355\n",
      "step 120 , total_loss: 1.2619, data_loss: 1.2619\n",
      "step 140 , total_loss: 1.2344, data_loss: 1.2344\n",
      "step 160 , total_loss: 1.3057, data_loss: 1.3057\n",
      "step 180 , total_loss: 1.3193, data_loss: 1.3193\n",
      "step 200 , total_loss: 1.3051, data_loss: 1.3051\n",
      "step 220 , total_loss: 1.2705, data_loss: 1.2705\n",
      "step 240 , total_loss: 1.2467, data_loss: 1.2467\n",
      "step 260 , total_loss: 1.3139, data_loss: 1.3139\n",
      "step 280 , total_loss: 1.2904, data_loss: 1.2904\n",
      "step 300 , total_loss: 1.1762, data_loss: 1.1762\n",
      "step 320 , total_loss: 1.3303, data_loss: 1.3303\n",
      "step 340 , total_loss: 1.2731, data_loss: 1.2731\n",
      "step 360 , total_loss: 1.1386, data_loss: 1.1386\n",
      "step 380 , total_loss: 1.3730, data_loss: 1.3730\n",
      "step 400 , total_loss: 1.1379, data_loss: 1.1379\n",
      "step 420 , total_loss: 1.1648, data_loss: 1.1648\n",
      "step 440 , total_loss: 1.2426, data_loss: 1.2426\n",
      "step 460 , total_loss: 1.2906, data_loss: 1.2906\n",
      "step 480 , total_loss: 1.1061, data_loss: 1.1061\n",
      "step 500 , total_loss: 1.1773, data_loss: 1.1773\n",
      "step 520 , total_loss: 1.1653, data_loss: 1.1653\n",
      "step 540 , total_loss: 1.1571, data_loss: 1.1571\n",
      "step 560 , total_loss: 1.2544, data_loss: 1.2544\n",
      "step 580 , total_loss: 1.2026, data_loss: 1.2026\n",
      "at epoch 6 train info: exp_var:0.0005578398704528809, mae:0.9855023, rmse:1.1944455, rsquare:-0.12894906457543343 eval info: exp_var:-0.002660393714904785, mae:0.98536015, rmse:1.1973304, rsquare:-0.12256318661885146\n",
      "at epoch 6 , train time: 13.3 eval time: 15.5\n",
      "step 20 , total_loss: 1.2189, data_loss: 1.2189\n",
      "step 40 , total_loss: 1.2027, data_loss: 1.2027\n",
      "step 60 , total_loss: 1.2383, data_loss: 1.2383\n",
      "step 80 , total_loss: 1.2512, data_loss: 1.2512\n",
      "step 100 , total_loss: 1.1862, data_loss: 1.1862\n",
      "step 120 , total_loss: 1.1173, data_loss: 1.1173\n",
      "step 140 , total_loss: 1.1263, data_loss: 1.1263\n",
      "step 160 , total_loss: 1.1544, data_loss: 1.1544\n",
      "step 180 , total_loss: 1.1682, data_loss: 1.1682\n",
      "step 200 , total_loss: 1.1948, data_loss: 1.1948\n",
      "step 220 , total_loss: 1.1411, data_loss: 1.1411\n",
      "step 240 , total_loss: 1.1240, data_loss: 1.1240\n",
      "step 260 , total_loss: 1.2006, data_loss: 1.2006\n",
      "step 280 , total_loss: 1.1669, data_loss: 1.1669\n",
      "step 300 , total_loss: 1.1048, data_loss: 1.1048\n",
      "step 320 , total_loss: 1.2215, data_loss: 1.2215\n",
      "step 340 , total_loss: 1.1877, data_loss: 1.1877\n",
      "step 360 , total_loss: 1.0643, data_loss: 1.0643\n",
      "step 380 , total_loss: 1.2786, data_loss: 1.2786\n",
      "step 400 , total_loss: 1.0319, data_loss: 1.0319\n",
      "step 420 , total_loss: 1.0566, data_loss: 1.0566\n",
      "step 440 , total_loss: 1.1571, data_loss: 1.1571\n",
      "step 460 , total_loss: 1.1893, data_loss: 1.1893\n",
      "step 480 , total_loss: 1.0224, data_loss: 1.0224\n",
      "step 500 , total_loss: 1.0818, data_loss: 1.0818\n",
      "step 520 , total_loss: 1.1227, data_loss: 1.1227\n",
      "step 540 , total_loss: 1.0838, data_loss: 1.0838\n",
      "step 560 , total_loss: 1.1893, data_loss: 1.1893\n",
      "step 580 , total_loss: 1.1235, data_loss: 1.1235\n",
      "at epoch 7 train info: exp_var:0.004230797290802002, mae:0.9581098, rmse:1.1340194, rsquare:-0.017560600389711523 eval info: exp_var:0.0005990862846374512, mae:0.962509, rmse:1.14, rsquare:-0.017677352474470664\n",
      "at epoch 7 , train time: 13.4 eval time: 15.1\n",
      "step 20 , total_loss: 1.1585, data_loss: 1.1585\n",
      "step 40 , total_loss: 1.1613, data_loss: 1.1613\n",
      "step 60 , total_loss: 1.1504, data_loss: 1.1504\n",
      "step 80 , total_loss: 1.2143, data_loss: 1.2143\n",
      "step 100 , total_loss: 1.1371, data_loss: 1.1371\n",
      "step 120 , total_loss: 1.0757, data_loss: 1.0757\n",
      "step 140 , total_loss: 1.1107, data_loss: 1.1107\n",
      "step 160 , total_loss: 1.1050, data_loss: 1.1050\n",
      "step 180 , total_loss: 1.1170, data_loss: 1.1170\n",
      "step 200 , total_loss: 1.1698, data_loss: 1.1698\n",
      "step 220 , total_loss: 1.1054, data_loss: 1.1054\n",
      "step 240 , total_loss: 1.0889, data_loss: 1.0889\n",
      "step 260 , total_loss: 1.1703, data_loss: 1.1703\n",
      "step 280 , total_loss: 1.1311, data_loss: 1.1311\n",
      "step 300 , total_loss: 1.1024, data_loss: 1.1024\n",
      "step 320 , total_loss: 1.1880, data_loss: 1.1880\n",
      "step 340 , total_loss: 1.1718, data_loss: 1.1718\n",
      "step 360 , total_loss: 1.0574, data_loss: 1.0574\n",
      "step 380 , total_loss: 1.2533, data_loss: 1.2533\n",
      "step 400 , total_loss: 1.0047, data_loss: 1.0047\n",
      "step 420 , total_loss: 1.0258, data_loss: 1.0258\n",
      "step 440 , total_loss: 1.1348, data_loss: 1.1348\n",
      "step 460 , total_loss: 1.1589, data_loss: 1.1589\n",
      "step 480 , total_loss: 1.0043, data_loss: 1.0043\n",
      "step 500 , total_loss: 1.0551, data_loss: 1.0551\n",
      "step 520 , total_loss: 1.1234, data_loss: 1.1234\n",
      "step 540 , total_loss: 1.0683, data_loss: 1.0683\n",
      "step 560 , total_loss: 1.1756, data_loss: 1.1756\n",
      "step 580 , total_loss: 1.1030, data_loss: 1.1030\n",
      "at epoch 8 train info: exp_var:0.005316197872161865, mae:0.9445987, rmse:1.1222299, rsquare:0.0034378850379346293 eval info: exp_var:0.0013664960861206055, mae:0.95126593, rmse:1.1298672, rsquare:0.0003740121811917074\n",
      "at epoch 8 , train time: 13.4 eval time: 15.4\n",
      "step 20 , total_loss: 1.1463, data_loss: 1.1463\n",
      "step 40 , total_loss: 1.1569, data_loss: 1.1569\n",
      "step 60 , total_loss: 1.1232, data_loss: 1.1232\n",
      "step 80 , total_loss: 1.2111, data_loss: 1.2111\n",
      "step 100 , total_loss: 1.1250, data_loss: 1.1250\n",
      "step 120 , total_loss: 1.0708, data_loss: 1.0708\n",
      "step 140 , total_loss: 1.1170, data_loss: 1.1170\n",
      "step 160 , total_loss: 1.0947, data_loss: 1.0947\n",
      "step 180 , total_loss: 1.1044, data_loss: 1.1044\n",
      "step 200 , total_loss: 1.1691, data_loss: 1.1691\n",
      "step 220 , total_loss: 1.0998, data_loss: 1.0998\n",
      "step 240 , total_loss: 1.0828, data_loss: 1.0828\n",
      "step 260 , total_loss: 1.1658, data_loss: 1.1658\n",
      "step 280 , total_loss: 1.1244, data_loss: 1.1244\n",
      "step 300 , total_loss: 1.1101, data_loss: 1.1101\n",
      "step 320 , total_loss: 1.1791, data_loss: 1.1791\n",
      "step 340 , total_loss: 1.1722, data_loss: 1.1722\n",
      "step 360 , total_loss: 1.0616, data_loss: 1.0616\n",
      "step 380 , total_loss: 1.2487, data_loss: 1.2487\n",
      "step 400 , total_loss: 1.0002, data_loss: 1.0002\n",
      "step 420 , total_loss: 1.0195, data_loss: 1.0195\n",
      "step 440 , total_loss: 1.1297, data_loss: 1.1297\n",
      "step 460 , total_loss: 1.1519, data_loss: 1.1519\n",
      "step 480 , total_loss: 1.0021, data_loss: 1.0021\n",
      "step 500 , total_loss: 1.0497, data_loss: 1.0497\n",
      "step 520 , total_loss: 1.1276, data_loss: 1.1276\n",
      "step 540 , total_loss: 1.0665, data_loss: 1.0665\n",
      "step 560 , total_loss: 1.1742, data_loss: 1.1742\n",
      "step 580 , total_loss: 1.0994, data_loss: 1.0994\n",
      "at epoch 9 train info: exp_var:0.005731642246246338, mae:0.93973523, rmse:1.1209817, rsquare:0.005688964677539765 eval info: exp_var:0.0016826987266540527, mae:0.9472242, rmse:1.1291146, rsquare:0.0016569967276028885\n",
      "at epoch 9 , train time: 13.4 eval time: 15.1\n",
      "step 20 , total_loss: 1.1449, data_loss: 1.1449\n",
      "step 40 , total_loss: 1.1576, data_loss: 1.1576\n",
      "step 60 , total_loss: 1.1157, data_loss: 1.1157\n",
      "step 80 , total_loss: 1.2121, data_loss: 1.2121\n",
      "step 100 , total_loss: 1.1222, data_loss: 1.1222\n",
      "step 120 , total_loss: 1.0716, data_loss: 1.0716\n",
      "step 140 , total_loss: 1.1209, data_loss: 1.1209\n",
      "step 160 , total_loss: 1.0926, data_loss: 1.0926\n",
      "step 180 , total_loss: 1.1012, data_loss: 1.1012\n",
      "step 200 , total_loss: 1.1698, data_loss: 1.1698\n",
      "step 220 , total_loss: 1.0990, data_loss: 1.0990\n",
      "step 240 , total_loss: 1.0823, data_loss: 1.0823\n",
      "step 260 , total_loss: 1.1654, data_loss: 1.1654\n",
      "step 280 , total_loss: 1.1234, data_loss: 1.1234\n",
      "step 300 , total_loss: 1.1133, data_loss: 1.1133\n",
      "step 320 , total_loss: 1.1762, data_loss: 1.1762\n",
      "step 340 , total_loss: 1.1730, data_loss: 1.1730\n",
      "step 360 , total_loss: 1.0635, data_loss: 1.0635\n",
      "step 380 , total_loss: 1.2480, data_loss: 1.2480\n",
      "step 400 , total_loss: 0.9997, data_loss: 0.9997\n",
      "step 420 , total_loss: 1.0186, data_loss: 1.0186\n",
      "step 440 , total_loss: 1.1282, data_loss: 1.1282\n",
      "step 460 , total_loss: 1.1506, data_loss: 1.1506\n",
      "step 480 , total_loss: 1.0020, data_loss: 1.0020\n",
      "step 500 , total_loss: 1.0488, data_loss: 1.0488\n",
      "step 520 , total_loss: 1.1290, data_loss: 1.1290\n",
      "step 540 , total_loss: 1.0664, data_loss: 1.0664\n",
      "step 560 , total_loss: 1.1744, data_loss: 1.1744\n",
      "step 580 , total_loss: 1.0994, data_loss: 1.0994\n",
      "at epoch 10 train info: exp_var:0.005963444709777832, mae:0.93850905, rmse:1.120848, rsquare:0.005959090361243202 eval info: exp_var:0.0019034743309020996, mae:0.94622046, rmse:1.1290704, rsquare:0.0017165149202872376\n",
      "at epoch 10 , train time: 13.4 eval time: 15.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<reco_utils.recommender.deeprec.models.xDeepFM.XDeepFMModel at 0x7f5cb2092320>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.1290704, 'exp_var': 0.0019034743309020996, 'rsquare': 0.0017165149202872376, 'mae': 0.94622046}\n"
     ]
    }
   ],
   "source": [
    "res_syn = model.run_eval(test_file)\n",
    "print(res_syn)\n",
    "#pm.record(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\\[1\\] Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., & Sun, G. (2018). xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems.Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, KDD 2018, London, UK, August 19-23, 2018.<br>\n",
    "\\[2\\] The Criteo datasets: http://labs.criteo.com/category/dataset/. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
