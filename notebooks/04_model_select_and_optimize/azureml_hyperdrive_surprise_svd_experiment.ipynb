{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br><br>\n",
    "# SVD Hyperparameter Tuning with Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to tune the hyperparameters of a matrix factorization algorithm by utilizing **Azure Machine Learning service** ([AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/)) in the context of movie recommendations. To use AzureML you will need an Azure subscription. We use the SVD algorithm from the Surprise library.\n",
    "\n",
    "We present the overall process of utilizing AzureML by demonstrating some key steps while avoiding too much detail. \n",
    "\n",
    "For more details about the **SVD** algorithm:\n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Original paper](http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)\n",
    "* [Surprise homepage](https://surprise.readthedocs.io/en/stable/)\n",
    "  \n",
    "Regarding **AzureML**, please refer to:\n",
    "* [Quickstart notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python)\n",
    "* [Hyperdrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import surprise\n",
    "import datetime as dt\n",
    "import papermill as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import rmse, precision_at_k, ndcg_at_k\n",
    "from reco_utils.recommender.surprise.surprise_utils import compute_rating_predictions, compute_ranking_predictions\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Surprise version: {}\".format(surprise.__version__))\n",
    "\n",
    "import azureml as aml\n",
    "import azureml.widgets\n",
    "import azureml.train.hyperdrive as hd\n",
    "\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that an AzureML workspace has already been created. For instructions how to do this, see [here](README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AzureML workspace info. Note, will look up \"aml_config\\config.json\" first, then fall back to using this\n",
    "SUBSCRIPTION_ID = '<subscription-id>'\n",
    "RESOURCE_GROUP  = '<resource-group>'\n",
    "WORKSPACE_NAME  = '<workspace-name>'\n",
    "\n",
    "# Connect to a workspace\n",
    "try:\n",
    "    ws = aml.core.Workspace.from_config()\n",
    "except aml.exceptions.UserErrorException:\n",
    "    try:\n",
    "        ws = aml.core.Workspace(\n",
    "            subscription_id=SUBSCRIPTION_ID,\n",
    "            resource_group=RESOURCE_GROUP,\n",
    "            workspace_name=WORKSPACE_NAME\n",
    "        )\n",
    "        ws.write_config()\n",
    "    except aml.exceptions.AuthenticationException:\n",
    "        ws = None\n",
    "\n",
    "if ws is None:\n",
    "    raise ValueError(\n",
    "        \"\"\"Cannot access the AzureML workspace w/ the config info provided.\n",
    "        Please check if you entered the correct id, group name and workspace name\"\"\"\n",
    "    )\n",
    "else:\n",
    "    print(\"AzureML workspace name: \", ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following cells, we\n",
    "1. Create a *remote compute target* (cpu_cluster) if it does not exist already,\n",
    "2. Mount a *data store* and upload the training set, and\n",
    "3. Run a hyperparameter tuning experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Remote Compute Target\n",
    "\n",
    "We create an AI Compute for our remote compute target. The script will load the cluster if it already exists. You can look at [this document](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets) to learn more about setting up a *compute target*.\n",
    "\n",
    "> Note: we create a low priority cluster to save costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Remote compute (cluster) configuration. If you want to save costs decrease these.\n",
    "# Each standard_D2_V2 VM has 2 vCPUs, 7GB memory, 100GB SSD storage\n",
    "\n",
    "VM_SIZE = 'STANDARD_D2_V2'\n",
    "VM_PRIORITY = 'dedicated' #'lowpriority'\n",
    "# Cluster nodes\n",
    "MIN_NODES = 8 #0\n",
    "MAX_NODES = 8\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpuclustersvd\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE, \n",
    "                                                           min_nodes=MIN_NODES, \n",
    "                                                           vm_priority=VM_PRIORITY,\n",
    "                                                           max_nodes=MAX_NODES)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(cpu_cluster.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Dataset\n",
    "1. Download data and split into training and testing sets\n",
    "2. Upload the training set to the default **blob storage** of the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\"]\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = python_random_split(data, ratio=[0.7, 0.15, 0.15], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'aml_data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "train.to_pickle(os.path.join(DATA_DIR, TRAIN_FILE_NAME))\n",
    "\n",
    "VAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_val.pkl\"\n",
    "validation.to_pickle(os.path.join(DATA_DIR, VAL_FILE_NAME))\n",
    "\n",
    "TEST_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_test.pkl\"\n",
    "test.to_pickle(os.path.join(DATA_DIR, TEST_FILE_NAME))\n",
    "\n",
    "# Note, all the files under DATA_DIR will be uploaded to the data store\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(\n",
    "    src_dir=DATA_DIR,\n",
    "    target_path='data',\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also prepare a training script [svd_training.py](../../reco_utils/azureml/svd_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to AzureML experiment so that we can track the metrics and optimize the primary metric via **hyperdrive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = 'aml_script'\n",
    "\n",
    "# Clean-up scripts if already exists\n",
    "shutil.rmtree(SCRIPT_DIR, ignore_errors=True)\n",
    "\n",
    "# Copy scripts to SCRIPT_DIR temporarly\n",
    "shutil.copytree(os.path.join('..', '..', 'reco_utils'), os.path.join(SCRIPT_DIR, 'reco_utils'))\n",
    "\n",
    "ENTRY_SCRIPT_NAME = 'reco_utils/azureml/svd_training.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a search space for the hyperparameters. All the parameter values will be passed to our training script.\n",
    "\n",
    "We specify the output directory as ./outputs. The outputs directory is specially treated by Azure ML in that all the content in this directory gets uploaded to the workspace as part of the run history. The files written to this directory are therefore accessible even once the remote run is over. In the training script (svd_training.py), we use the output directory for saving the trained models. \n",
    "\n",
    "AzureML hyperdrive provides `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each approach are beyond the scope of this notebook and can be found in [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the Bayesian sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_svd_model\"\n",
    "PRIMARY_METRIC = 'precision_at_k'\n",
    "RATING_METRICS = ['rmse']\n",
    "RANKING_METRICS = ['precision_at_k', 'ndcg_at_k']  \n",
    "USERCOL = 'userID'\n",
    "ITEMCOL = 'itemID'\n",
    "RECOMMEND_SEEN = False\n",
    "K = 10\n",
    "RANDOM_STATE = 0\n",
    "VERBOSE = True\n",
    "NUM_EPOCHS = 30\n",
    "BIASED = True\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ds.as_mount(),\n",
    "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n",
    "    '--validation-datapath': \"data/\" + VAL_FILE_NAME,\n",
    "    '--output_dir': './outputs',\n",
    "    '--surprise-reader': 'ml-100k',\n",
    "    '--rating-metrics': RATING_METRICS,\n",
    "    '--ranking-metrics': RANKING_METRICS,\n",
    "    '--usercol': USERCOL,\n",
    "    '--itemcol': ITEMCOL,\n",
    "    '--k': str(K),\n",
    "    '--random-state': str(RANDOM_STATE),\n",
    "    '--epochs': str(NUM_EPOCHS),\n",
    "}\n",
    "\n",
    "if BIASED:\n",
    "    script_params['--biased'] = ''\n",
    "if VERBOSE:\n",
    "    script_params['--verbose'] = ''\n",
    "if RECOMMEND_SEEN:\n",
    "    script_params['--recommend-seen'] = ''\n",
    "    \n",
    "# hyperparameters search space\n",
    "# We do not set 'lr_all' and 'reg_all' because they will be overwritten by the other lr_ and reg_ parameters\n",
    "\n",
    "hyper_params = {\n",
    "    '--n_factors': hd.choice(10, 50, 100, 150, 200),\n",
    "    '--init_mean': hd.uniform(-0.5, 0.5),\n",
    "    '--init_std_dev': hd.uniform(0.01, 0.2),\n",
    "    '--lr_bu': hd.uniform(1e-6, 0.1), \n",
    "    '--lr_bi': hd.uniform(1e-6, 0.1), \n",
    "    '--lr_pu': hd.uniform(1e-6, 0.1), \n",
    "    '--lr_qi': hd.uniform(1e-6, 0.1), \n",
    "    '--reg_bu': hd.uniform(1e-6, 1),\n",
    "    '--reg_bi': hd.uniform(1e-6, 1), \n",
    "    '--reg_pu': hd.uniform(1e-6, 1), \n",
    "    '--reg_qi': hd.uniform(1e-6, 1)\n",
    "}\n",
    "\n",
    "# Note, BayesianParameterSampling only support choice, uniform, and quniform\n",
    "ps = hd.BayesianParameterSampling(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you submit the experiment, you can see the progress from the notebook by using `azureml.widgets.RunDetails`. You can directly check the details from the Azure portal as well. To get the link, run `run.get_portal_url()`.\n",
    "\n",
    "For RandomSampling, you can use early termnination policy\n",
    "```\n",
    "policy = hd.BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=3)\n",
    "```\n",
    "\n",
    "> Since we will do hyperparameter tuning, we create a `HyperDriveRunConfig` and pass it to the experiment object. If you already know what hyperparameters to use and still want to utilize AzureML for other purposes (e.g. model management), you can set the hyperparameter values directly to `script_params` and run the experiment, `run = exp.submit(est)`, instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperdrive experimentation configuration\n",
    "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search for the best hyperparameters. \n",
    "MAX_CONCURRENT_RUNS = 8\n",
    "\n",
    "est = azureml.train.estimator.Estimator(\n",
    "    source_directory=SCRIPT_DIR,\n",
    "    entry_script=ENTRY_SCRIPT_NAME,\n",
    "    script_params=script_params,\n",
    "    compute_target=cpu_cluster,\n",
    "    conda_packages=['pandas', 'scikit-learn'],\n",
    "    pip_packages=['scikit-surprise', 'psutil']\n",
    ")\n",
    "\n",
    "hd_config = hd.HyperDriveRunConfig(\n",
    "    estimator=est, \n",
    "    hyperparameter_sampling=ps,\n",
    "    primary_metric_name=PRIMARY_METRIC,\n",
    "    primary_metric_goal=hd.PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=MAX_TOTAL_RUNS,\n",
    "    max_concurrent_runs=MAX_CONCURRENT_RUNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute Runs in AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an experiment to track the runs in the workspace\n",
    "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)\n",
    "run = exp.submit(config=hd_config)\n",
    "\n",
    "azureml.widgets.RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the experiment progress from this notebook by using `azureml.widgets.RunDetails(hd_run).show()` or check from the Azure portal with the url link you can get by running `hd_run.get_portal_url()`.\n",
    "To load an existing Hyperdrive run, use `hd_run = hd.HyperDriveRun(exp, <user-run-id>, hyperdrive_run_config=hd_run_config)`. You also can cancel a run with `hd_run.cancel()`.\n",
    "![](https://recodatasets.blob.core.windows.net/images/svd_hyperdrive1.PNG)\n",
    "![](https://recodatasets.blob.core.windows.net/images/svd_hyperdrive2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Download Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_start_time = run.get_details()['startTimeUtc']\n",
    "LOG_DIR = os.path.join('./log', parent_start_time)\n",
    "CHILD_LOG_DIR = os.path.join(LOG_DIR, 'child_runs')\n",
    "if not os.path.isdir(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "if not os.path.isdir(CHILD_LOG_DIR):\n",
    "    os.makedirs(CHILD_LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_log(local_path, file_name, log_info):\n",
    "    with open(os.path.join(local_path, file_name), 'w') as fp:\n",
    "        json.dump(log_info, fp)\n",
    "\n",
    "parent_run_id = run.get_details()['runId']\n",
    "dump_log(LOG_DIR, 'parent_run_details_' + parent_run_id + '.json', run.get_details())\n",
    "dump_log(LOG_DIR, 'parent_run_metrics_' + parent_run_id + '.json', run.get_metrics())\n",
    "for child_run in run.get_children():\n",
    "    child_run_details = child_run.get_details()\n",
    "    child_run_id = child_run_details['runId']\n",
    "    dump_log(CHILD_LOG_DIR, 'child_run_details_' + child_run_id + '.json', child_run_details)\n",
    "    dump_log(CHILD_LOG_DIR, 'child_run_metrics_' + child_run_id + '.json', child_run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and printout metrics\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\" \".join(parameter_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the metrics on the test data. To do this, get the SVD model that was saved as model.dump in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('aml_model', exist_ok=True)\n",
    "best_run.download_file('outputs/model.dump', output_file_path='aml_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = surprise.dump.load('aml_model/model.dump')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "predictions = compute_rating_predictions(svd, test, usercol=\"userID\", itemcol=\"itemID\")\n",
    "for metric in RATING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, predictions)\n",
    "\n",
    "all_predictions = compute_ranking_predictions(svd, train, usercol=\"userID\", itemcol=\"itemID\", remove_seen=RECOMMEND_SEEN)\n",
    "for metric in RANKING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, all_predictions, col_prediction='prediction', k=K)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children_metric_endtime(run, metric_name, report_extra=True):\n",
    "    \"\"\"Get the start/end time and primary metric of each child run.\n",
    "    \"\"\"\n",
    "    column_names = [\"parent_run_id\", \n",
    "                    \"child_run_id\", \n",
    "                    metric_name,\n",
    "                    \"parent_start_time\",\n",
    "                    \"parent_end_time\",\n",
    "                    \"child_start_time\", \n",
    "                    \"child_end_time\"]\n",
    "    if report_extra:\n",
    "        extra_column_names = [\"main_script_start_time\",\n",
    "                              \"main_script_end_time\",\n",
    "                              \"data_loading_start_time\",\n",
    "                              \"data_loading_end_time\",\n",
    "                              \"train_script_start_time\",\n",
    "                              \"train_script_end_time\"]\n",
    "        column_names += extra_column_names\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "    run_details = run.get_details()\n",
    "    for idx, child_run in enumerate(run.get_children()):\n",
    "        child_run_details = child_run.get_details()\n",
    "        child_run_metrics = child_run.get_metrics()\n",
    "        if child_run_details[\"properties\"][\"azureml.runsource\"] == \"experiment\":\n",
    "            parent_run_id = run_details[\"runId\"]\n",
    "            child_run_id = child_run_details[\"runId\"]\n",
    "            primary_metric = child_run_metrics[metric_name]\n",
    "            parent_start_time = run_details[\"startTimeUtc\"]\n",
    "            parent_end_time = run_details[\"endTimeUtc\"]\n",
    "            child_start_time = child_run_details[\"startTimeUtc\"]\n",
    "            child_end_time = child_run_details[\"endTimeUtc\"]\n",
    "            row = [parent_run_id, \n",
    "                   child_run_id, \n",
    "                   primary_metric,\n",
    "                   parent_start_time,\n",
    "                   parent_end_time,\n",
    "                   child_start_time, \n",
    "                   child_end_time]\n",
    "            if report_extra:\n",
    "                for timestamp_name in extra_column_names:\n",
    "                    row.append(child_run_metrics[timestamp_name])\n",
    "                    \n",
    "            df.loc[idx] = row\n",
    "\n",
    "    return df\n",
    "\n",
    "results = get_children_metric_endtime(run, PRIMARY_METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_FORMAT = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "results[\"child_end_time\"] = pd.to_datetime(results[\"child_end_time\"], format=TIMESTAMP_FORMAT)\n",
    "parent_start_time = dt.datetime.strptime(parent_start_time, TIMESTAMP_FORMAT)\n",
    "results[\"time_since_start\"] = results.apply(lambda row: (row[\"child_end_time\"] - parent_start_time).total_seconds(), axis=1)\n",
    "results.sort_values(by=[\"time_since_start\"], inplace=True)\n",
    "results[\"best_metric_so_far\"] = results[metric_name].cummax()\n",
    "\n",
    "results['best_metric'] = best_run_metrics[PRIMARY_METRIC]\n",
    "results['best_parameters'] = \" \".join(parameter_values)\n",
    "results['test_metric'] = test_results[PRIMARY_METRIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"child_start_time\"] = pd.to_datetime(results[\"child_start_time\"], format=TIMESTAMP_FORMAT)\n",
    "results[\"parent_start_time\"] = pd.to_datetime(results[\"parent_start_time\"], format=TIMESTAMP_FORMAT)\n",
    "results[\"parent_end_time\"] = pd.to_datetime(results[\"parent_end_time\"], format=TIMESTAMP_FORMAT)\n",
    "\n",
    "results[\"total_time\"] = (results.iloc[0][\"parent_end_time\"] - results.iloc[0][\"parent_start_time\"]).total_seconds()\n",
    "results[\"execution_time\"] = (max(results[\"child_end_time\"]) - min(results[\"child_start_time\"])).total_seconds()\n",
    "results[\"deployment_time\"] = (min(results[\"child_start_time\"]) - results.iloc[0][\"parent_start_time\"]).total_seconds()\n",
    "results[\"clean_up_time\"] = (results.iloc[0][\"parent_end_time\"] - max(results[\"child_end_time\"])).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to csv file\n",
    "result_path = os.path.join(LOG_DIR, \"results.csv\")\n",
    "results.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. time\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"precision_at_k\")\n",
    "plt.xlim((0, max(results[\"time_since_start\"])))\n",
    "plt.ylim((0, 0.1))\n",
    "plt.title(\"HyperDrive\")\n",
    "plt.step(results[\"time_since_start\"], results[\"best_metric_so_far\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_bar(data, series_labels, category_labels=None, \n",
    "                show_values=False, value_format=\"{}\", y_label=None, \n",
    "                grid=True, reverse=False):\n",
    "    \"\"\"Plots a stacked bar chart with the data and labels provided.\n",
    "    Copied from https://stackoverflow.com/questions/44309507/stacked-bar-plot-using-matplotlib\n",
    "\n",
    "    Keyword arguments:\n",
    "    data            -- 2-dimensional numpy array or nested list\n",
    "                       containing data for each series in rows\n",
    "    series_labels   -- list of series labels (these appear in\n",
    "                       the legend)\n",
    "    category_labels -- list of category labels (these appear\n",
    "                       on the x-axis)\n",
    "    show_values     -- If True then numeric value labels will \n",
    "                       be shown on each bar\n",
    "    value_format    -- Format string for numeric value labels\n",
    "                       (default is \"{}\")\n",
    "    y_label         -- Label for y-axis (str)\n",
    "    grid            -- If True display grid\n",
    "    reverse         -- If True reverse the order that the\n",
    "                       series are displayed (left-to-right\n",
    "                       or right-to-left)\n",
    "    \"\"\"\n",
    "\n",
    "    ny = len(data[0])\n",
    "    ind = list(range(ny))\n",
    "\n",
    "    axes = []\n",
    "    cum_size = np.zeros(ny)\n",
    "\n",
    "    data = np.array(data)\n",
    "\n",
    "    if reverse:\n",
    "        data = np.flip(data, axis=1)\n",
    "        category_labels = reversed(category_labels)\n",
    "\n",
    "    for i, row_data in enumerate(data):\n",
    "        axes.append(plt.bar(ind, row_data, bottom=cum_size, \n",
    "                            label=series_labels[i]))\n",
    "        cum_size += row_data\n",
    "\n",
    "    if category_labels:\n",
    "        plt.xticks(ind, category_labels)\n",
    "\n",
    "    if y_label:\n",
    "        plt.ylabel(y_label)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    if grid:\n",
    "        plt.grid()\n",
    "\n",
    "    if show_values:\n",
    "        for axis in axes:\n",
    "            for bar in axis:\n",
    "                w, h = bar.get_width(), bar.get_height()\n",
    "                plt.text(bar.get_x() + w/2, bar.get_y() + h/2, \n",
    "                         value_format.format(h), ha=\"center\", \n",
    "                         va=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "series_labels = ['deployment', 'execution', 'clean-up']\n",
    "\n",
    "data = [\n",
    "    [results.iloc[0]['deployment_time']], \n",
    "    [results.iloc[0]['execution_time']], \n",
    "    [results.iloc[0]['clean_up_time']]\n",
    "]\n",
    "\n",
    "category_labels = ['HyperDrive']\n",
    "\n",
    "stacked_bar(\n",
    "    data, \n",
    "    series_labels, \n",
    "    category_labels=category_labels, \n",
    "    show_values=True, \n",
    "    value_format=\"{:.1f}\",\n",
    "    y_label=\"Quantity (units)\",\n",
    "    grid=False\n",
    ")\n",
    "\n",
    "#plt.savefig('bar.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(SCRIPT_DIR)\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "except (PermissionError, FileNotFoundError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Concluding Remarks\n",
    "\n",
    "We showed how to tune **all** the hyperparameters accepted by Surprise SVD simultaneously, by utilizing the Azure Machine Learning service. \n",
    "For example, training and evaluation of a single SVD model takes about 50 seconds on the 100k MovieLens data on a Standard D2_V2 VM. Searching through 100 different combinations of hyperparameters sequentially would take about 80 minutes whereas this notebook took less than half that. With AzureML, one can easily specify the size of the cluster according to the problem at hand and use Bayesian sampling to navigate efficiently through a large space of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Matrix factorization algorithms in Surprise](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) \n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n",
    "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco)",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
