{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Collected Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the following metrics for each experiment:\n",
    "  * total running time\n",
    "  * deployment time (first run's start time - job submission time)\n",
    "  * clean-up time (job finish time - last run's finish time)\n",
    "  * best metric on validation set\n",
    "  * metric on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import papermill as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = os.path.join('./log_old')\n",
    "COLUMN_NAMES = [\"best_metric\", \"test_metric\", \"total_time\", \"deployment_time\", \"clean_up_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "\n",
    "for i in range(5):\n",
    "    print('Experiment ' + str(i))\n",
    "    original_notebook_path = os.path.join(notebook_dir, 'azureml_hyperdrive_surprise_svd_experiment.ipynb')\n",
    "    output_notebook_path = os.path.join(notebook_dir, 'output.ipynb')\n",
    "    pm.execute_notebook(original_notebook_path, output_notebook_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(result_dir, column_names, only_first_row=True):\n",
    "    \"\"\"Combine results of all the repeated experiments.\n",
    "    \"\"\"\n",
    "    results_all = pd.DataFrame(columns=[\"experiment_id\"] + column_names)\n",
    "    for idx, sub_folder in enumerate(next(os.walk(result_dir))[1]):\n",
    "        result_file_path = os.path.join(result_dir, sub_folder, \"results.csv\")\n",
    "        try:\n",
    "            results = pd.read_csv(result_file_path)\n",
    "        except:\n",
    "            continue\n",
    "        results[\"experiment_id\"] = idx\n",
    "        results = results[[\"experiment_id\"] + column_names]\n",
    "        if only_first_row:\n",
    "            results_all.loc[idx] = results.iloc[0]\n",
    "        else:\n",
    "            results_all = pd.concat([results_all, results])\n",
    "    results_all.reset_index(drop=True, inplace=True)\n",
    "    return results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = combine_results(RESULT_DIR, COLUMN_NAMES)\n",
    "results_mean = results_all[COLUMN_NAMES].mean(axis=0).to_dict()\n",
    "results_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy vs. time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = combine_results(RESULT_DIR, [\"time_since_start\", \"best_metric_so_far\"], only_first_row=False)\n",
    "results_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_groupby = results_all.groupby([\"experiment_id\"]).min()\n",
    "min_time_since_start = results_groupby[\"time_since_start\"].mean()\n",
    "min_time_since_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_groupby = results_all.groupby([\"experiment_id\"]).max()\n",
    "max_time_since_start = results_groupby[\"time_since_start\"].mean()\n",
    "max_time_since_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_times = list(set(results_all[\"time_since_start\"]))\n",
    "unique_times = [x for x in unique_times if x > min_time_since_start and x < max_time_since_start]\n",
    "unique_times.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric_so_far(x, time_since_start):\n",
    "    return x[x[\"time_since_start\"] <= time_since_start][\"best_metric_so_far\"].max()\n",
    "\n",
    "avg_best_metric = []\n",
    "for t in unique_times:\n",
    "    avg_best_metric.append(results_all.groupby([\"experiment_id\"]).apply(lambda x: get_best_metric_so_far(x, t)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"precision_at_k\")\n",
    "plt.xlim((0, max(unique_times)))\n",
    "plt.ylim((0, 0.1))\n",
    "plt.title(\"HyperDrive\")\n",
    "plt.step(unique_times, avg_best_metric)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
