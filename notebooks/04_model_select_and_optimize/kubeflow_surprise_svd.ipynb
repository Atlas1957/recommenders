{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br><br>\n",
    "# SVD Hyperparameter Tuning with Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to tune the hyperparameters of a matrix factorization algorithm, SVD (Singular Value Decomposition) from the Surprise library, by utilizing **[Kubeflow](https://www.kubeflow.org/)** in the context of movie recommendations. Kubeflow is a machine learning toolkit for [Kubernetes](https://kubernetes.io/) which makes deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.\n",
    "\n",
    "We present the overall process of deploying Kubeflow on [AKS (Azure Kubernetes Service)](https://azure.microsoft.com/en-us/services/kubernetes-service/) and utilize it to run hyperparameter tuning experiments by demonstrating some key steps while avoiding too much detail. \n",
    "\n",
    "For more details about the **SVD** algorithm:\n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Original paper](http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)\n",
    "* [Surprise homepage](https://surprise.readthedocs.io/en/stable/)\n",
    "  \n",
    "Regarding **Kubeflow**, please refer to:\n",
    "* [Azure Kubeflow labs github repo](https://github.com/Azure/kubeflow-labs)\n",
    "* [Kubeflow official doc: Getting started on Kubernetes](https://www.kubeflow.org/docs/started/getting-started-k8s/)\n",
    "* [Hyperparameter tuning a Tensorflow model on Kubeflow with GPU cluster](https://github.com/loomlike/hyperparameter-tuning-on-kubernetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites\n",
    "\n",
    "* Docker (if you want to create your own docker image) - To install, see [docker site](https://docs.docker.com/install/).\n",
    "* Azure CLI - The easiest way is to use [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/).\n",
    "  - You need the Azure CLI version 2.0.64 or later installed and configured. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest#update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Surprise version: 1.0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import surprise\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import rmse, precision_at_k, ndcg_at_k\n",
    "from reco_utils.recommender.surprise.surprise_utils import compute_rating_predictions, compute_ranking_predictions\n",
    "from reco_utils.common.constants import SEED\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Surprise version: {}\".format(surprise.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "#### 1.1 AKS setup\n",
    "To create AKS and cluster, first make sure you signed in to use Azure CLI with a correct subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the subscription, run `az account set --subscription <YOUR-SUBSCRIPTION-NAME-OR-ID>`.\n",
    "\n",
    "Then, run following commands to create a resource group and AKS cluster.\n",
    "This example will create four `Standard_D2_v2` (CPU VM) nodes for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RG_NAME = \"junminaks-cpu\"  # \"<YOUR-RESOURCE-GROUP-NAME>\"\n",
    "AKS_NAME = \"junminaks-cpu\"  # \"<RESOURCE-NAME>\"\n",
    "LOCATION = \"eastus\"  # \"<RESOURCE-REGION>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourceGroups/junminaks-cpu\",\n",
      "  \"location\": \"eastus\",\n",
      "  \"managedBy\": null,\n",
      "  \"name\": \"junminaks-cpu\",\n",
      "  \"properties\": {\n",
      "    \"provisioningState\": \"Succeeded\"\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": null\n",
      "}\n",
      "\u001b[K{- Finished ..ion done[############################################]  100.0000%\n",
      "  \"aadProfile\": null,\n",
      "  \"addonProfiles\": {\n",
      "    \"omsagent\": {\n",
      "      \"config\": {\n",
      "        \"logAnalyticsWorkspaceResourceID\": \"/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourcegroups/defaultresourcegroup-eus/providers/microsoft.operationalinsights/workspaces/defaultworkspace-03909a66-bef8-4d52-8e9a-a346604e0902-eus\"\n",
      "      },\n",
      "      \"enabled\": true\n",
      "    }\n",
      "  },\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"availabilityZones\": null,\n",
      "      \"count\": 4,\n",
      "      \"enableAutoScaling\": null,\n",
      "      \"maxCount\": null,\n",
      "      \"maxPods\": 110,\n",
      "      \"minCount\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"orchestratorVersion\": \"1.12.8\",\n",
      "      \"osDiskSizeGb\": 100,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"provisioningState\": \"Succeeded\",\n",
      "      \"type\": \"AvailabilitySet\",\n",
      "      \"vmSize\": \"Standard_D2_v2\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"apiServerAuthorizedIpRanges\": null,\n",
      "  \"dnsPrefix\": \"junminaks--junminaks-cpu-03909a\",\n",
      "  \"enablePodSecurityPolicy\": null,\n",
      "  \"enableRbac\": true,\n",
      "  \"fqdn\": \"junminaks--junminaks-cpu-03909a-380d8be0.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourcegroups/junminaks-cpu/providers/Microsoft.ContainerService/managedClusters/junminaks-cpu\",\n",
      "  \"kubernetesVersion\": \"1.12.8\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCrMEhu8Ve8mbQxrxFUoJ8qusCZJsxyxbIXAjfkFnpZUkhR4cfbJ+x2rjoRUbnikLLgOBZ1d6E58Q5fp3oHRklnG8U05Z4/Y5nnoKpbVPo2BUFruZcEpP/GeEqRosZm5UZ6tMjielEYt/W4uEtPqrVClGlh61MdFL/y74CwikR7yD978s3SgIB5QV/6s7zNn4rjCXh3iIFIVY1TjzbIAizWYzVTm2WzE98HWs5BWcdtYCDVrIJ45ueS6PkCCV6NgDM2MOyVS92YXhjxNJU/lihqeARIN75XmvIDd4CuDV5gjpA7scQycWMF/XUyD6d/FqhNXB3UpW8Toa+G9Nf8oqQf jun@Juns-MacBook-Pro.local\\n\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"junminaks-cpu\",\n",
      "  \"networkProfile\": {\n",
      "    \"dnsServiceIp\": \"10.0.0.10\",\n",
      "    \"dockerBridgeCidr\": \"172.17.0.1/16\",\n",
      "    \"networkPlugin\": \"kubenet\",\n",
      "    \"networkPolicy\": null,\n",
      "    \"podCidr\": \"10.244.0.0/16\",\n",
      "    \"serviceCidr\": \"10.0.0.0/16\"\n",
      "  },\n",
      "  \"nodeResourceGroup\": \"MC_junminaks-cpu_junminaks-cpu_eastus\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"junminaks-cpu\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"24260096-fd77-4396-aa10-ff2ede624eaf\",\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create resource group\n",
    "!az group create --name {RG_NAME} --location {LOCATION}\n",
    "\n",
    "# Create AKS cluster\n",
    "!az aks create \\\n",
    "    --resource-group {RG_NAME} \\\n",
    "    --name {AKS_NAME} \\\n",
    "    --node-count 4 \\\n",
    "    --node-vm-size Standard_D2_v2 \\\n",
    "    --enable-addons monitoring \\\n",
    "    --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an AKS cluster may take few minutes. If the creation is successful, you'll see something like:\n",
    "```\n",
    "{- Finished ..ion done[############################################]  100.0000%\n",
    "  \"aadProfile\": null,\n",
    "    \"addonProfiles\": {\n",
    "      \"omsagent\": {\n",
    "        \"config\": {\n",
    "  ...\n",
    "```\n",
    "\n",
    "Now, install Kubernetes CLI `kubectl` for running commands against the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading client to \"/usr/local/bin/kubectl\" from \"https://storage.googleapis.com/kubernetes-release/release/v1.14.3/bin/darwin/amd64/kubectl\"\u001b[0m\n",
      "\u001b[33mPlease ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az aks install-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"junminaks-cpu\" as current context in /Users/jun/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials --resource-group {RG_NAME} --name {AKS_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error, check if you have a write permission for `~/.kube/config` file.\n",
    "\n",
    "To verify connection, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS   ROLES   AGE    VERSION\r\n",
      "aks-nodepool1-30917087-0   Ready    agent   100m   v1.12.8\r\n",
      "aks-nodepool1-30917087-1   Ready    agent   101m   v1.12.8\r\n",
      "aks-nodepool1-30917087-2   Ready    agent   101m   v1.12.8\r\n",
      "aks-nodepool1-30917087-3   Ready    agent   101m   v1.12.8\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If connection is successful, the nodes information will be printed out like:\n",
    "```\n",
    "NAME                       STATUS    ROLES     AGE       VERSION\n",
    "aks-nodepool1-17965807-0   Ready     agent     11m       v1.12.8\n",
    "...\n",
    "```\n",
    "\n",
    "#### 1.2 Kubeflow setup\n",
    "Kubeflow makes use of *[ksonnet](https://www.kubeflow.org/docs/components/ksonnet/)* to help manage deployments.\n",
    "\n",
    "First, setup environment variables and download the ksonnet file by running the following scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OS_TYPE\"] = \"linux\"  # Use \"darwin\" for Mac\n",
    "os.environ[\"KS_VER\"] = \"0.13.1\"\n",
    "os.environ[\"KS_PKG\"] = \"ks_{0}_{1}_amd64\".format(os.environ[\"KS_VER\"], os.environ[\"OS_TYPE\"])\n",
    "os.environ[\"PATH\"] = \"{0}:{1}/bin/{2}\".format(\n",
    "    os.environ[\"PATH\"],\n",
    "    os.environ[\"HOME\"],\n",
    "    os.environ[\"KS_PKG\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ks_0.13.1_darwin_amd64/CHANGELOG.md\n",
      "x ks_0.13.1_darwin_amd64/CODE-OF-CONDUCT.md\n",
      "x ks_0.13.1_darwin_amd64/CONTRIBUTING.md\n",
      "x ks_0.13.1_darwin_amd64/LICENSE\n",
      "x ks_0.13.1_darwin_amd64/README.md\n",
      "x ks_0.13.1_darwin_amd64/ks\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O /tmp/${KS_PKG}.tar.gz https://github.com/ksonnet/ksonnet/releases/download/v${KS_VER}/${KS_PKG}.tar.gz -q\n",
    "mkdir -p ${HOME}/bin\n",
    "tar -xvf /tmp/${KS_PKG}.tar.gz -C ${HOME}/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download, extract Kubeflow and deploy it. For more details about this process, please see [installation instruction of Kubeflow on Azure](https://www.kubeflow.org/docs/azure/deploy/install-kubeflow/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KFCTL_VER\"] = \"0.5.1\"\n",
    "os.environ[\"KFCTL_PKG\"] = \"kfctl_v{}_{}\".format(os.environ[\"KFCTL_VER\"], os.environ[\"OS_TYPE\"])\n",
    "os.environ[\"PATH\"] = \"{0}:{1}/bin\".format(\n",
    "    os.environ[\"PATH\"],\n",
    "    os.environ[\"HOME\"]\n",
    ")\n",
    "os.environ['KFAPP'] = \"kfapp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ./kfctl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O /tmp/${KFCTL_PKG}.tar.gz https://github.com/kubeflow/kubeflow/releases/download/v${KFCTL_VER}/${KFCTL_PKG}.tar.gz -q\n",
    "tar -xvf /tmp/${KFCTL_PKG}.tar.gz -C ${HOME}/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "kfctl init ${KFAPP}\n",
    "cd ${KFAPP}\n",
    "kfctl generate k8s\n",
    "kfctl apply k8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the deployment, check kubeflow pods as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                       READY   STATUS    RESTARTS   AGE\r\n",
      "ambassador-7b8477f667-96mmt                                1/1     Running   0          2m45s\r\n",
      "ambassador-7b8477f667-kbchr                                1/1     Running   0          2m45s\r\n",
      "ambassador-7b8477f667-mswcr                                1/1     Running   0          2m45s\r\n",
      "argo-ui-9cbd45fdf-sgm6k                                    1/1     Running   0          2m37s\r\n",
      "centraldashboard-796c755dcf-v6ctn                          1/1     Running   0          2m30s\r\n",
      "jupyter-web-app-589f8756c9-rvvlx                           1/1     Running   0          2m22s\r\n",
      "katib-ui-7c6997fd96-rqkdw                                  1/1     Running   0          2m11s\r\n",
      "metacontroller-0                                           1/1     Running   0          2m3s\r\n",
      "minio-594df758b9-sqnfc                                     1/1     Running   0          110s\r\n",
      "ml-pipeline-75b5d4585-dqwzb                                1/1     Running   0          108s\r\n",
      "ml-pipeline-persistenceagent-7ffd468c4b-blr59              1/1     Running   0          106s\r\n",
      "ml-pipeline-scheduledworkflow-56c8f5bc9b-62kgv             1/1     Running   0          107s\r\n",
      "ml-pipeline-ui-858f7f979d-7cpk2                            1/1     Running   0          104s\r\n",
      "ml-pipeline-viewer-controller-deployment-cc7fb8dfd-djrvm   1/1     Running   0          105s\r\n",
      "mysql-5d5b5475c4-d6n46                                     1/1     Running   0          109s\r\n",
      "notebooks-controller-685db44f8c-wc2hp                      1/1     Running   0          117s\r\n",
      "pytorch-operator-9996bcb49-9jpp2                           1/1     Running   0          96s\r\n",
      "studyjob-controller-57cb6746ff-49d8s                       1/1     Running   0          2m10s\r\n",
      "tensorboard-76dffc9ffc-7rhfc                               1/1     Running   0          90s\r\n",
      "tf-job-dashboard-84bdddd5cc-gmzxk                          1/1     Running   0          83s\r\n",
      "tf-job-operator-8486555578-922zj                           1/1     Running   0          83s\r\n",
      "vizier-core-bcc86677d-58xm9                                1/1     Running   1          2m10s\r\n",
      "vizier-core-rest-68c7577f84-xp6h2                          1/1     Running   0          2m11s\r\n",
      "vizier-db-54f46c46c6-gvqj2                                 1/1     Running   0          2m10s\r\n",
      "vizier-suggestion-bayesianoptimization-97f4f76dd-hvbkj     1/1     Running   0          2m11s\r\n",
      "vizier-suggestion-grid-6f94f98f9d-5n2qp                    1/1     Running   0          2m11s\r\n",
      "vizier-suggestion-hyperband-68f4cc7f5d-7xbm7               1/1     Running   0          2m11s\r\n",
      "vizier-suggestion-random-6ff5b8f6d8-9qj5m                  1/1     Running   0          2m10s\r\n",
      "workflow-controller-d5cb6468d-zcj5k                        1/1     Running   0          2m37s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the namespace to be \"kubeflow\" so that we don't need to use `-n kubeflow` argument for every `kubectl` command in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"junminaks-cpu\" modified.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context $AKS_NAME --namespace=kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Persistent volumn setup\n",
    "One last thing we should do before moving to the next step is to create a persistent volumn to store our dataset. A PersistentVolumeClaim (PVC) is a request for storage by a user. For details, see [persistent volumes with Azure files](https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv). Here, we create 10G size storage, which is defined in *[reco_utils/kubernetes/manifest/azure-file-pvc.yaml](../../reco_utils/kubernetes/manifest/azure-file-pvc.yaml)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storageclass.storage.k8s.io/azurefile created\n",
      "clusterrole.rbac.authorization.k8s.io/system:azure-cloud-provider created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/system:azure-cloud-provider created\n",
      "persistentvolumeclaim/azurefile created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/azure-file-sc.yaml\n",
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/azure-pvc-roles.yaml\n",
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/azure-file-pvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the deployment, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\r\n",
      "azurefile   Bound    pvc-3086e6ad-8e16-11e9-9ec7-46b2371e9153   10Gi       RWX            azurefile      45s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc azurefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "1. Download data and split into training, validation and testing sets\n",
    "2. Upload the training and validation sets to our PVC. To do that,\n",
    "  1. Attach a pod to the PVC\n",
    "  2. Copy the datasets onto the pod\n",
    "  3. Delete the pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:00<00:00, 6.38kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0     196     242     3.0\n",
       "1     186     302     3.0\n",
       "2      22     377     1.0\n",
       "3     244      51     2.0\n",
       "4     166     346     1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\"]\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = python_random_split(data, [0.7, 0.15, 0.15], seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "train_pickle_path = os.path.join(tmpdir.name, TRAIN_FILE_NAME)\n",
    "train.to_pickle(train_pickle_path)\n",
    "\n",
    "VAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_val.pkl\"\n",
    "val_pickle_path = os.path.join(tmpdir.name, VAL_FILE_NAME)\n",
    "validation.to_pickle(val_pickle_path)\n",
    "\n",
    "TEST_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_test.pkl\"\n",
    "test_pickle_path = os.path.join(tmpdir.name, TEST_FILE_NAME)\n",
    "test.to_pickle(test_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create [pvc-loader](../../reco_utils/kubernetes/manifest/pvc-loader.yaml) to upload the datasets into `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pvc-loader\" deleted\n",
      "pod/pvc-loader created\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod pvc-loader  # Delete if the pod already exists\n",
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/pvc-loader.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl cp {train_pickle_path} pvc-loader:/data/\n",
    "!kubectl cp {val_pickle_path} pvc-loader:/data/\n",
    "!kubectl cp {test_pickle_path} pvc-loader:/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movielens_100k_test.pkl\r\n",
      "movielens_100k_train.pkl\r\n",
      "movielens_100k_val.pkl\r\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "!kubectl exec pvc-loader -- bash -c \"ls /data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pvc-loader\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "# We don't need this pod anymore\n",
    "!kubectl delete pod pvc-loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Training scripts\n",
    "\n",
    "We also prepare a training script [svd_training.py](../../reco_utils/kubernetes/svd_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to Katib so that we can track the metrics and optimize the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a search space for the hyperparameters. All the parameter values will be passed to our training script.\n",
    "\n",
    "We specify the output directory as ./outputs. The outputs directory is specially treated by Azure ML in that all the content in this directory gets uploaded to the workspace as part of the run history. The files written to this directory are therefore accessible even once the remote run is over. In the training script (svd_training.py), we use the output directory for saving the trained models. \n",
    "\n",
    "AzureML hyperdrive provides `RandomParameterSampling`, `GridParameterSampling`, and `BayesianParameterSampling`. Details about each approach are beyond the scope of this notebook and can be found in [Azure doc](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the Bayesian sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_svd_model\"\n",
    "PRIMARY_METRIC = 'precision_at_k'\n",
    "RATING_METRICS = ['rmse']\n",
    "RANKING_METRICS = ['precision_at_k', 'ndcg_at_k']  \n",
    "USERCOL = 'userID'\n",
    "ITEMCOL = 'itemID'\n",
    "RECOMMEND_SEEN = False\n",
    "K = 10\n",
    "RANDOM_STATE = 0\n",
    "VERBOSE = True\n",
    "NUM_EPOCHS = 30\n",
    "BIASED = True\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': ds.as_mount(),\n",
    "    '--train-datapath': \"data/\" + TRAIN_FILE_NAME,\n",
    "    '--validation-datapath': \"data/\" + VAL_FILE_NAME,\n",
    "    '--output_dir': './outputs',\n",
    "    '--surprise-reader': 'ml-100k',\n",
    "    '--rating-metrics': RATING_METRICS,\n",
    "    '--ranking-metrics': RANKING_METRICS,\n",
    "    '--usercol': USERCOL,\n",
    "    '--itemcol': ITEMCOL,\n",
    "    '--k': K,\n",
    "    '--random-state': RANDOM_STATE,\n",
    "    '--epochs': NUM_EPOCHS,\n",
    "}\n",
    "\n",
    "if BIASED:\n",
    "    script_params['--biased'] = ''\n",
    "if VERBOSE:\n",
    "    script_params['--verbose'] = ''\n",
    "if RECOMMEND_SEEN:\n",
    "    script_params['--recommend-seen'] = ''\n",
    "    \n",
    "# hyperparameters search space\n",
    "# We do not set 'lr_all' and 'reg_all' because they will be overwritten by the other lr_ and reg_ parameters\n",
    "\n",
    "hyper_params = {\n",
    "    'n_factors': hd.choice(10, 50, 100, 150, 200),\n",
    "    'init_mean': hd.uniform(-0.5, 0.5),\n",
    "    'init_std_dev': hd.uniform(0.01, 0.2),\n",
    "    'lr_bu': hd.uniform(1e-6, 0.1), \n",
    "    'lr_bi': hd.uniform(1e-6, 0.1), \n",
    "    'lr_pu': hd.uniform(1e-6, 0.1), \n",
    "    'lr_qi': hd.uniform(1e-6, 0.1), \n",
    "    'reg_bu': hd.uniform(1e-6, 1),\n",
    "    'reg_bi': hd.uniform(1e-6, 1), \n",
    "    'reg_pu': hd.uniform(1e-6, 1), \n",
    "    'reg_qi': hd.uniform(1e-6, 1)\n",
    "}\n",
    "\n",
    "# Note, BayesianParameterSampling only support choice, uniform, and quniform\n",
    "ps = hd.BayesianParameterSampling(hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you submit the experiment, you can see the progress from the notebook by using `azureml.widgets.RunDetails`. You can directly check the details from the Azure portal as well. To get the link, run `run.get_portal_url()`.\n",
    "\n",
    "For RandomSampling, you can use early termnination policy\n",
    "```\n",
    "policy = hd.BanditPolicy(evaluation_interval=1, slack_factor=0.1, delay_evaluation=3)\n",
    "```\n",
    "\n",
    "> Since we will do hyperparameter tuning, we create a `HyperDriveRunConfig` and pass it to the experiment object. If you already know what hyperparameters to use and still want to utilize AzureML for other purposes (e.g. model management), you can set the hyperparameter values directly to `script_params` and run the experiment, `run = exp.submit(est)`, instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperdrive experimentation configuration\n",
    "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search for the best hyperparameters. \n",
    "MAX_CONCURRENT_RUNS = 8\n",
    "\n",
    "est = azureml.train.estimator.Estimator(\n",
    "    source_directory=SCRIPT_DIR,\n",
    "    entry_script=ENTRY_SCRIPT_NAME,\n",
    "    script_params=script_params,\n",
    "    compute_target=cpu_cluster,\n",
    "    conda_packages=['pandas', 'scikit-learn'],\n",
    "    pip_packages=['scikit-surprise']\n",
    ")\n",
    "\n",
    "hd_config = hd.HyperDriveRunConfig(\n",
    "    estimator=est, \n",
    "    hyperparameter_sampling=ps,\n",
    "    primary_metric_name=PRIMARY_METRIC,\n",
    "    primary_metric_goal=hd.PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=MAX_TOTAL_RUNS,\n",
    "    max_concurrent_runs=MAX_CONCURRENT_RUNS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute Runs in AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cd3c66d91f433888ffffc6e45b46fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: movielens_100k_svd_model_1551957798853\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: movielens_100k_svd_model_1551957798853\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'movielens_100k_svd_model_1551957798853',\n",
       " 'target': 'cpuclustersvd',\n",
       " 'status': 'Completed',\n",
       " 'endTimeUtc': '2019-03-07T11:57:23.000Z',\n",
       " 'properties': {'primary_metric_config': '{\"name\": \"precision_at_k\", \"goal\": \"maximize\"}',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive'},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://anargyri6699545766.blob.core.windows.net/azureml/ExperimentRun/dcid.movielens_100k_svd_model_1551957798853/azureml-logs/hyperdrive.txt?sv=2018-03-28&sr=b&sig=5vpEl9OQZvTgU4v%2BiIAKATIHBJg8UyBw8e3jD0xtAz4%3D&st=2019-03-07T11%3A47%3A26Z&se=2019-03-07T19%3A57%3A26Z&sp=r'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313732b33930494d8245cf305ffd99f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an experiment to track the runs in the workspace\n",
    "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)\n",
    "run = exp.submit(config=hd_config)\n",
    "\n",
    "azureml.widgets.RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the experiment progress from this notebook by using `azureml.widgets.RunDetails(hd_run).show()` or check from the Azure portal with the url link you can get by running `hd_run.get_portal_url()`.\n",
    "To load an existing Hyperdrive run, use `hd_run = hd.HyperDriveRun(exp, <user-run-id>, hyperdrive_run_config=hd_run_config)`. You also can cancel a run with `hd_run.cancel()`.\n",
    "![](https://recodatasets.blob.core.windows.net/images/svd_hyperdrive1.PNG)\n",
    "![](https://recodatasets.blob.core.windows.net/images/svd_hyperdrive2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and printout metrics\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['Arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of epochs': 30,\n",
       " 'rmse': 1.0343498081373697,\n",
       " 'precision_at_k': 0.10000000000000002,\n",
       " 'ndcg_at_k': 0.11498322243961594}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--datastore $AZUREML_DATAREFERENCE_workspaceblobstore --train-datapath data/movielens_100k_train.pkl --validation-datapath data/movielens_100k_val.pkl --output_dir ./outputs --surprise-reader ml-100k --rating-metrics rmse --ranking-metrics precision_at_k ndcg_at_k --usercol userID --itemcol itemID --k 10 --random-state 0 --epochs 30 --biased --verbose --n_factors 150 --init_mean -0.4163305768968 --init_std_dev 0.159711436379793 --lr_bu 0.0386753983834255 --lr_bi 4.48660045721016E-05 --lr_pu 0.0119378772073106 --lr_qi 0.0936873305814469 --reg_bu 0.385400397115581 --reg_bi 0.975251474623207 --reg_pu 0.906537637834819 --reg_qi 0.801240951271603\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(parameter_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the metrics on the test data. To do this, get the SVD model that was saved as model.dump in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('aml_model', exist_ok=True)\n",
    "best_run.download_file('outputs/model.dump', output_file_path='aml_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = surprise.dump.load('aml_model/model.dump')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.0331492610799313, 'precision_at_k': 0.09968017057569298, 'ndcg_at_k': 0.1160964958978592}\n"
     ]
    }
   ],
   "source": [
    "test_results = {}\n",
    "predictions = compute_rating_predictions(svd, test, usercol=\"userID\", itemcol=\"itemID\")\n",
    "for metric in RATING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, predictions)\n",
    "\n",
    "all_predictions = compute_ranking_predictions(svd, train, usercol=\"userID\", itemcol=\"itemID\", recommend_seen=RECOMMEND_SEEN)\n",
    "for metric in RANKING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, all_predictions, col_prediction='prediction', k=K)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "To uninstall Kubeflow,\n",
    "```\n",
    "cd ${KF_APP}\n",
    "# If you want to delete all the resources, including storage.\n",
    "kfctl delete all --delete_storage\n",
    "# If you want to preserve storage, which contains metadata and information\n",
    "# from mlpipeline.\n",
    "kfctl delete all\n",
    "```\n",
    "\n",
    "To remove AKS cluster,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Concluding Remarks\n",
    "\n",
    "We showed how to tune **all** the hyperparameters accepted by Surprise SVD simultaneously, by utilizing the Azure Machine Learning service. \n",
    "For example, training and evaluation of a single SVD model takes about 50 seconds on the 100k MovieLens data on a Standard D2_V2 VM. Searching through 100 different combinations of hyperparameters sequentially would take about 80 minutes whereas this notebook took less than half that. With AzureML, one can easily specify the size of the cluster according to the problem at hand and use Bayesian sampling to navigate efficiently through a large space of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Matrix factorization algorithms in Surprise](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) \n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n",
    "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco_base",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
