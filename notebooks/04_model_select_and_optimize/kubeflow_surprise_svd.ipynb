{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br><br>\n",
    "# SVD Hyperparameter Tuning with Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to tune the hyperparameters of a matrix factorization algorithm, SVD (Singular Value Decomposition) from the Surprise library, by utilizing **[Kubeflow](https://www.kubeflow.org/)** in the context of movie recommendations. Kubeflow is a machine learning toolkit for [Kubernetes](https://kubernetes.io/) which makes deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.\n",
    "\n",
    "We present the overall process of deploying Kubeflow on [AKS (Azure Kubernetes Service)](https://azure.microsoft.com/en-us/services/kubernetes-service/) and utilize it to run hyperparameter tuning experiments by demonstrating some key steps while avoiding too much detail. \n",
    "\n",
    "For more details about the **SVD** algorithm:\n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Original paper](http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)\n",
    "* [Surprise homepage](https://surprise.readthedocs.io/en/stable/)\n",
    "  \n",
    "Regarding **Kubeflow**, please refer to:\n",
    "* [Azure Kubeflow labs github repo](https://github.com/Azure/kubeflow-labs)\n",
    "* [Kubeflow official doc: Getting started on Kubernetes](https://www.kubeflow.org/docs/started/getting-started-k8s/)\n",
    "* [Hyperparameter tuning a Tensorflow model on Kubeflow with GPU cluster](https://github.com/loomlike/hyperparameter-tuning-on-kubernetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "* Docker (if you want to create your own docker image) - To install, see [docker site](https://docs.docker.com/install/).\n",
    "* Azure CLI - The easiest way is to use [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/).\n",
    "  - You need the Azure CLI version 2.0.64 or later installed and configured. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest#update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Surprise version: 1.0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import surprise\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "\n",
    "from reco_utils.common.constants import SEED\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import rmse, precision_at_k, ndcg_at_k\n",
    "from reco_utils.recommender.surprise.surprise_utils import compute_rating_predictions, compute_ranking_predictions\n",
    "from reco_utils.kubeflow.manifest.utils import (\n",
    "    choice,\n",
    "    uniform,\n",
    "    make_hypertune_manifest,\n",
    "    worker_manifest,\n",
    ")\n",
    "from reco_utils.kubeflow.manifest import (\n",
    "    Goal,\n",
    "    SearchType,\n",
    "    WorkerType,\n",
    ")\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Surprise version: {}\".format(surprise.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "During the setup, we create AKS cluster and install Kubeflow on it.\n",
    "\n",
    "#### 1.1 AKS setup\n",
    "To create AKS and cluster, first make sure you signed in to use Azure CLI with a correct subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the subscription, run `az account set --subscription <YOUR-SUBSCRIPTION-NAME-OR-ID>`.\n",
    "\n",
    "Next, **set desired names for your resource group and AKS as well as the region you want to create the resources at** to the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RG_NAME = \"reco-aks-rg\"  # YOUR-RESOURCE-GROUP-NAME\n",
    "AKS_NAME = \"reco-aks\"    # RESOURCE-NAME\n",
    "LOCATION = \"eastus\"      # RESOURCE-REGION. To get all the available region, run 'az account list-locations' and see 'name' key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following commands to create the resources. This example will create four [Standard_D2_v2](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-general#dv2-series) CPU VM nodes for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create resource group\n",
    "!az group create --name {RG_NAME} --location {LOCATION}\n",
    "\n",
    "# Create AKS cluster\n",
    "!az aks create \\\n",
    "    --resource-group {RG_NAME} \\\n",
    "    --name {AKS_NAME} \\\n",
    "    --node-count 4 \\\n",
    "    --node-vm-size Standard_D2_v2 \\\n",
    "    --enable-addons monitoring \\\n",
    "    --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an AKS cluster may take few minutes. If the creation is successful, you'll see something like:\n",
    "```\n",
    "{- Finished ..ion done[############################################]  100.0000%\n",
    "  \"aadProfile\": null,\n",
    "    \"addonProfiles\": {\n",
    "      \"omsagent\": {\n",
    "        \"config\": {\n",
    "  ...\n",
    "```\n",
    "\n",
    "Now, install [Kubernetes CLI](https://docs.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster#install-the-kubernetes-cli) `kubectl` for running commands against the cluster. If you already installed kubectl, skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az aks install-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the CLI to your cluster by runing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az aks get-credentials --resource-group {RG_NAME} --name {AKS_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error, check if you have read/write permissions on the kubernetes config file. In a linux machine, the file will be at `~/.kube/config`.\n",
    "\n",
    "To verify the connection of CLI to your cluster, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS   ROLES   AGE   VERSION\n",
      "aks-nodepool1-30917087-0   Ready    agent   23h   v1.12.8\n",
      "aks-nodepool1-30917087-1   Ready    agent   23h   v1.12.8\n",
      "aks-nodepool1-30917087-2   Ready    agent   23h   v1.12.8\n",
      "aks-nodepool1-30917087-3   Ready    agent   23h   v1.12.8\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the connection is successful, the nodes information will be printed out like:\n",
    "```\n",
    "NAME                       STATUS    ROLES     AGE       VERSION\n",
    "aks-nodepool1-17965807-0   Ready     agent     11m       v1.12.8\n",
    "...\n",
    "```\n",
    "\n",
    "#### 1.2 Kubeflow setup\n",
    "Kubeflow makes use of *[ksonnet](https://www.kubeflow.org/docs/components/ksonnet/)* to help manage deployments.\n",
    "\n",
    "First, setup environment variables and download the ksonnet file by running the following scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OS_TYPE\"] = \"linux\"  # Use \"darwin\" for Mac\n",
    "os.environ[\"KS_VER\"] = \"0.13.1\"\n",
    "os.environ[\"KS_PKG\"] = \"ks_{0}_{1}_amd64\".format(os.environ[\"KS_VER\"], os.environ[\"OS_TYPE\"])\n",
    "os.environ[\"PATH\"] = \"{0}:{1}/bin/{2}\".format(\n",
    "    os.environ[\"PATH\"],\n",
    "    os.environ[\"HOME\"],\n",
    "    os.environ[\"KS_PKG\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ks_0.13.1_darwin_amd64/CHANGELOG.md\n",
      "x ks_0.13.1_darwin_amd64/CODE-OF-CONDUCT.md\n",
      "x ks_0.13.1_darwin_amd64/CONTRIBUTING.md\n",
      "x ks_0.13.1_darwin_amd64/LICENSE\n",
      "x ks_0.13.1_darwin_amd64/README.md\n",
      "x ks_0.13.1_darwin_amd64/ks\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O /tmp/${KS_PKG}.tar.gz https://github.com/ksonnet/ksonnet/releases/download/v${KS_VER}/${KS_PKG}.tar.gz -q\n",
    "mkdir -p ${HOME}/bin\n",
    "tar -xvf /tmp/${KS_PKG}.tar.gz -C ${HOME}/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download, extract Kubeflow and deploy it. For more details about this process, please see [installation instruction of Kubeflow on Azure](https://www.kubeflow.org/docs/azure/deploy/install-kubeflow/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KFCTL_VER\"] = \"0.5.1\"\n",
    "os.environ[\"KFCTL_PKG\"] = \"kfctl_v{}_{}\".format(os.environ[\"KFCTL_VER\"], os.environ[\"OS_TYPE\"])\n",
    "os.environ[\"PATH\"] = \"{0}:{1}/bin\".format(\n",
    "    os.environ[\"PATH\"],\n",
    "    os.environ[\"HOME\"]\n",
    ")\n",
    "os.environ['KFAPP'] = \"kfapp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ./kfctl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O /tmp/${KFCTL_PKG}.tar.gz https://github.com/kubeflow/kubeflow/releases/download/v${KFCTL_VER}/${KFCTL_PKG}.tar.gz -q\n",
    "tar -xvf /tmp/${KFCTL_PKG}.tar.gz -C ${HOME}/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "kfctl init ${KFAPP}\n",
    "cd ${KFAPP}\n",
    "kfctl generate k8s\n",
    "kfctl apply k8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the deployment, check kubeflow pods as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                       READY   STATUS    RESTARTS   AGE\n",
      "ambassador-7b8477f667-96mmt                                1/1     Running   0          21h\n",
      "ambassador-7b8477f667-kbchr                                1/1     Running   0          21h\n",
      "ambassador-7b8477f667-mswcr                                1/1     Running   0          21h\n",
      "argo-ui-9cbd45fdf-sgm6k                                    1/1     Running   0          21h\n",
      "centraldashboard-796c755dcf-v6ctn                          1/1     Running   0          21h\n",
      "jupyter-web-app-589f8756c9-rvvlx                           1/1     Running   0          21h\n",
      "katib-ui-7c6997fd96-rqkdw                                  1/1     Running   0          21h\n",
      "metacontroller-0                                           1/1     Running   0          21h\n",
      "minio-594df758b9-sqnfc                                     1/1     Running   0          21h\n",
      "ml-pipeline-75b5d4585-dqwzb                                1/1     Running   0          21h\n",
      "ml-pipeline-persistenceagent-7ffd468c4b-blr59              1/1     Running   0          21h\n",
      "ml-pipeline-scheduledworkflow-56c8f5bc9b-62kgv             1/1     Running   0          21h\n",
      "ml-pipeline-ui-858f7f979d-7cpk2                            1/1     Running   0          21h\n",
      "ml-pipeline-viewer-controller-deployment-cc7fb8dfd-djrvm   1/1     Running   0          21h\n",
      "mysql-5d5b5475c4-d6n46                                     1/1     Running   0          21h\n",
      "notebooks-controller-685db44f8c-wc2hp                      1/1     Running   0          21h\n",
      "pytorch-operator-9996bcb49-9jpp2                           1/1     Running   0          21h\n",
      "studyjob-controller-57cb6746ff-49d8s                       1/1     Running   0          21h\n",
      "tensorboard-76dffc9ffc-7rhfc                               1/1     Running   0          21h\n",
      "tf-job-dashboard-84bdddd5cc-gmzxk                          1/1     Running   0          21h\n",
      "tf-job-operator-8486555578-922zj                           1/1     Running   0          21h\n",
      "vizier-core-bcc86677d-58xm9                                1/1     Running   2          21h\n",
      "vizier-core-rest-68c7577f84-xp6h2                          1/1     Running   0          21h\n",
      "vizier-db-54f46c46c6-gvqj2                                 1/1     Running   0          21h\n",
      "vizier-suggestion-bayesianoptimization-97f4f76dd-hvbkj     1/1     Running   0          21h\n",
      "vizier-suggestion-grid-6f94f98f9d-5n2qp                    1/1     Running   0          21h\n",
      "vizier-suggestion-hyperband-68f4cc7f5d-7xbm7               1/1     Running   0          21h\n",
      "vizier-suggestion-random-6ff5b8f6d8-9qj5m                  1/1     Running   0          21h\n",
      "workflow-controller-d5cb6468d-zcj5k                        1/1     Running   0          21h\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the namespace to be \"kubeflow\" so that we don't need to use `-n kubeflow` argument for every *kubectl* command in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"junminaks-cpu\" modified.\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context {AKS_NAME} --namespace=kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Persistent volumn setup\n",
    "One last thing we should do before moving to the next step is to create a persistent volumn to store our dataset. A PersistentVolumeClaim (PVC) is a request for storage by a user. For details, see [persistent volumes with Azure files](https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv). Here, we create 10G size storage, which is defined in *[reco_utils/kubernetes/manifest/azure-file-pvc.yaml](../../reco_utils/kubernetes/manifest/azure-file-pvc.yaml)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storageclass.storage.k8s.io/azurefile created\n",
      "clusterrole.rbac.authorization.k8s.io/system:azure-cloud-provider created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/system:azure-cloud-provider created\n",
      "persistentvolumeclaim/azurefile created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/azure-file-sc.yaml\n",
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/azure-pvc-roles.yaml\n",
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/azure-file-pvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the deployment, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n",
      "azurefile   Bound    pvc-3086e6ad-8e16-11e9-9ec7-46b2371e9153   10Gi       RWX            azurefile      20h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc azurefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "1. Download data and split into training, validation and testing sets\n",
    "2. Upload the training and validation sets to our PVC. To do that,\n",
    "  1. Attach a pod to the PVC\n",
    "  2. Copy the datasets onto the pod\n",
    "  3. Delete the pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "VAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_val.pkl\"\n",
    "TEST_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_test.pkl\"\n",
    "\n",
    "USERCOL = 'userID'\n",
    "ITEMCOL = 'itemID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:00<00:00, 6.38kKB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0     196     242     3.0\n",
       "1     186     302     3.0\n",
       "2      22     377     1.0\n",
       "3     244      51     2.0\n",
       "4     166     346     1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\"]\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = python_random_split(data, [0.7, 0.15, 0.15], seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "train_pickle_path = os.path.join(tmpdir.name, TRAIN_FILE_NAME)\n",
    "train.to_pickle(train_pickle_path)\n",
    "\n",
    "val_pickle_path = os.path.join(tmpdir.name, VAL_FILE_NAME)\n",
    "validation.to_pickle(val_pickle_path)\n",
    "\n",
    "test_pickle_path = os.path.join(tmpdir.name, TEST_FILE_NAME)\n",
    "test.to_pickle(test_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create [pvc-loader](../../reco_utils/kubernetes/manifest/pvc-loader.yaml) to upload the datasets into `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pvc-loader\" deleted\n",
      "pod/pvc-loader created\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod pvc-loader  # Delete if the pod already exists\n",
    "!kubectl apply -f ../../reco_utils/kubernetes/manifest/pvc-loader.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl cp {train_pickle_path} pvc-loader:/data/\n",
    "!kubectl cp {val_pickle_path} pvc-loader:/data/\n",
    "!kubectl cp {test_pickle_path} pvc-loader:/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movielens_100k_test.pkl\r\n",
      "movielens_100k_train.pkl\r\n",
      "movielens_100k_val.pkl\r\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "!kubectl exec pvc-loader -- bash -c \"ls /data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the data, we don't need the pod anymore, so remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pvc-loader\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod pvc-loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Training scripts\n",
    "\n",
    "We prepare a training script [svd_training.py](../../reco_utils/kubernetes/svd_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to Katib so that we can track the metrics and optimize the primary metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a search space for the hyperparameters. All the parameter values will be passed to our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_svd_model\"\n",
    "PRIMARY_METRIC = 'precision_at_k'\n",
    "PRIMARY_METRIC_GOAL = Goal.MAXIMIZE\n",
    "IDEAL_METRIC_VALUE = 1.0\n",
    "RATING_METRICS = ['rmse']\n",
    "RANKING_METRICS = ['precision_at_k', 'ndcg_at_k']  \n",
    "METRICS = RATING_METRICS + RANKING_METRICS\n",
    "\n",
    "REMOVE_SEEN = True\n",
    "K = 10\n",
    "RANDOM_STATE = 0\n",
    "VERBOSE = True\n",
    "NUM_EPOCHS = 30\n",
    "BIASED = True\n",
    "\n",
    "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search for the best hyperparameters. \n",
    "MAX_CONCURRENT_RUNS = 8\n",
    "\n",
    "# TODO verify\n",
    "script_params = {\n",
    "    '--datastore': \"/data\",  # Our PVC will be mounted on \"/data\"\n",
    "    '--train-datapath': TRAIN_FILE_NAME,\n",
    "    '--validation-datapath': VAL_FILE_NAME,\n",
    "    '--output-dir': \"outputs\",\n",
    "    '--surprise-reader': \"ml-100k\",\n",
    "    '--rating-metrics': RATING_METRICS,\n",
    "    '--ranking-metrics': RANKING_METRICS,\n",
    "    '--usercol': USERCOL,\n",
    "    '--itemcol': ITEMCOL,\n",
    "    '--k': K,\n",
    "    '--random-state': RANDOM_STATE,\n",
    "    '--epochs': NUM_EPOCHS,\n",
    "}\n",
    "\n",
    "if BIASED:\n",
    "    script_params['--biased'] = ''\n",
    "if VERBOSE:\n",
    "    script_params['--verbose'] = ''\n",
    "if REMOVE_SEEN:\n",
    "    script_params['--remove-seen'] = ''\n",
    "\n",
    "# hyperparameters search space\n",
    "# We do not set 'lr_all' and 'reg_all' because they will be overwritten by the other lr_ and reg_ parameters\n",
    "hyperparams = {\n",
    "    '--n-factors': choice([10, 50, 100, 150, 200]),\n",
    "    '--init-mean': uniform(-0.5, 0.5),\n",
    "    '--init-std-dev': uniform(0.01, 0.2),\n",
    "    '--lr-bu': uniform(1e-6, 0.1), \n",
    "    '--lr-bi': uniform(1e-6, 0.1), \n",
    "    '--lr-pu': uniform(1e-6, 0.1), \n",
    "    '--lr-qi': uniform(1e-6, 0.1), \n",
    "    '--reg-bu': uniform(1e-6, 1),\n",
    "    '--reg-bi': uniform(1e-6, 1), \n",
    "    '--reg-pu': uniform(1e-6, 1), \n",
    "    '--reg-qi': uniform(1e-6, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudyJob spec has been generated. To start, run 'kubectl create -f jobs/movielens_100k_svd_model-1.yaml'\n"
     ]
    }
   ],
   "source": [
    "worker_spec = worker_manifest(\n",
    "    worker_type=WorkerType.WORKER,\n",
    "    image_name='loomlike/reco',\n",
    "    entry_script='app/reco_utils/kubeflow/svd_training.py',\n",
    "    params=script_params,\n",
    "    is_hypertune=True,\n",
    "    storage_path='/data',\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "studyjob_name, studyjob_file = make_hypertune_manifest(\n",
    "    study_name=EXP_NAME,\n",
    "    tag=1,\n",
    "    search_type=SearchType.RANDOM,\n",
    "    total_runs=MAX_TOTAL_RUNS,\n",
    "    concurrent_runs=MAX_CONCURRENT_RUNS,\n",
    "    primary_metric=PRIMARY_METRIC,\n",
    "    goal=PRIMARY_METRIC_GOAL,\n",
    "    ideal_metric_value=IDEAL_METRIC_VALUE,\n",
    "    metrics=METRICS,\n",
    "    hyperparams=hyperparams,\n",
    "    worker_spec=worker_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute Runs in AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cd3c66d91f433888ffffc6e45b46fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: movielens_100k_svd_model_1551957798853\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: movielens_100k_svd_model_1551957798853\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'movielens_100k_svd_model_1551957798853',\n",
       " 'target': 'cpuclustersvd',\n",
       " 'status': 'Completed',\n",
       " 'endTimeUtc': '2019-03-07T11:57:23.000Z',\n",
       " 'properties': {'primary_metric_config': '{\"name\": \"precision_at_k\", \"goal\": \"maximize\"}',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive'},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://anargyri6699545766.blob.core.windows.net/azureml/ExperimentRun/dcid.movielens_100k_svd_model_1551957798853/azureml-logs/hyperdrive.txt?sv=2018-03-28&sr=b&sig=5vpEl9OQZvTgU4v%2BiIAKATIHBJg8UyBw8e3jD0xtAz4%3D&st=2019-03-07T11%3A47%3A26Z&se=2019-03-07T19%3A57%3A26Z&sp=r'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313732b33930494d8245cf305ffd99f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an experiment to track the runs in the workspace\n",
    "exp = aml.core.Experiment(workspace=ws, name=EXP_NAME)\n",
    "run = exp.submit(config=hd_config)\n",
    "\n",
    "azureml.widgets.RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the experiment progress from this notebook by using `azureml.widgets.RunDetails(hd_run).show()` or check from the Azure portal with the url link you can get by running `hd_run.get_portal_url()`.\n",
    "To load an existing Hyperdrive run, use `hd_run = hd.HyperDriveRun(exp, <user-run-id>, hyperdrive_run_config=hd_run_config)`. You also can cancel a run with `hd_run.cancel()`.\n",
    "![](https://recodatasets.blob.core.windows.net/images/svd_hyperdrive1.PNG)\n",
    "![](https://recodatasets.blob.core.windows.net/images/svd_hyperdrive2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and printout metrics\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['Arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of epochs': 30,\n",
       " 'rmse': 1.0343498081373697,\n",
       " 'precision_at_k': 0.10000000000000002,\n",
       " 'ndcg_at_k': 0.11498322243961594}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--datastore $AZUREML_DATAREFERENCE_workspaceblobstore --train-datapath data/movielens_100k_train.pkl --validation-datapath data/movielens_100k_val.pkl --output_dir ./outputs --surprise-reader ml-100k --rating-metrics rmse --ranking-metrics precision_at_k ndcg_at_k --usercol userID --itemcol itemID --k 10 --random-state 0 --epochs 30 --biased --verbose --n_factors 150 --init_mean -0.4163305768968 --init_std_dev 0.159711436379793 --lr_bu 0.0386753983834255 --lr_bi 4.48660045721016E-05 --lr_pu 0.0119378772073106 --lr_qi 0.0936873305814469 --reg_bu 0.385400397115581 --reg_bi 0.975251474623207 --reg_pu 0.906537637834819 --reg_qi 0.801240951271603\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(parameter_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the metrics on the test data. To do this, get the SVD model that was saved as model.dump in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('aml_model', exist_ok=True)\n",
    "best_run.download_file('outputs/model.dump', output_file_path='aml_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = surprise.dump.load('aml_model/model.dump')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.0331492610799313, 'precision_at_k': 0.09968017057569298, 'ndcg_at_k': 0.1160964958978592}\n"
     ]
    }
   ],
   "source": [
    "test_results = {}\n",
    "predictions = compute_rating_predictions(svd, test, usercol=\"userID\", itemcol=\"itemID\")\n",
    "for metric in RATING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, predictions)\n",
    "\n",
    "all_predictions = compute_ranking_predictions(svd, train, usercol=\"userID\", itemcol=\"itemID\", recommend_seen=RECOMMEND_SEEN)\n",
    "for metric in RANKING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, all_predictions, col_prediction='prediction', k=K)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "To uninstall Kubeflow,\n",
    "```\n",
    "cd ${KF_APP}\n",
    "# If you want to delete all the resources, including storage.\n",
    "kfctl delete all --delete_storage\n",
    "# If you want to preserve storage, which contains metadata and information\n",
    "# from mlpipeline.\n",
    "kfctl delete all\n",
    "```\n",
    "\n",
    "To remove AKS cluster,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Concluding Remarks\n",
    "\n",
    "We showed how to tune **all** the hyperparameters accepted by Surprise SVD simultaneously, by utilizing the Azure Machine Learning service. \n",
    "For example, training and evaluation of a single SVD model takes about 50 seconds on the 100k MovieLens data on a Standard D2_V2 VM. Searching through 100 different combinations of hyperparameters sequentially would take about 80 minutes whereas this notebook took less than half that. With AzureML, one can easily specify the size of the cluster according to the problem at hand and use Bayesian sampling to navigate efficiently through a large space of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Matrix factorization algorithms in Surprise](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) \n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Fine-tune natural language processing models using Azure Machine Learning service](https://azure.microsoft.com/en-us/blog/fine-tune-natural-language-processing-models-using-azure-machine-learning-service/)\n",
    "* [Training, hyperparameter tune, and deploy with TensorFlow](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco_base",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
