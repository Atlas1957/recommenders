{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.<br>\n",
    "Licensed under the MIT License.</i>\n",
    "<br><br>\n",
    "# SVD Hyperparameter Tuning with Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to tune the hyperparameters of a matrix factorization algorithm, SVD (Singular Value Decomposition) from the Surprise library, by utilizing [**Kubeflow**](https://www.kubeflow.org/) in the context of movie recommendations. Kubeflow is a machine learning toolkit for [Kubernetes](https://kubernetes.io/) which makes deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.\n",
    "\n",
    "We present the overall process of deploying Kubeflow on [AKS (Azure Kubernetes Service)](https://azure.microsoft.com/en-us/services/kubernetes-service/) and use [**Katib**](https://www.kubeflow.org/docs/components/hyperparameter/) (a Kubeflow component dedicated to hyperparameter tuning tasks) to run hyperparameter tuning experiments by demonstrating some key steps while avoiding too much detail. \n",
    "\n",
    "For more details about the **SVD** algorithm:\n",
    "* [Surprise SVD deep-dive notebook](../02_model/surprise_svd_deep_dive.ipynb)\n",
    "* [Original paper](http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)\n",
    "* [Surprise homepage](https://surprise.readthedocs.io/en/stable/)\n",
    "  \n",
    "Regarding **Kubeflow**, please refer to:\n",
    "* [Azure Kubeflow labs github repo](https://github.com/Azure/kubeflow-labs)\n",
    "* [Kubeflow official doc: Getting started on Kubernetes](https://www.kubeflow.org/docs/started/getting-started-k8s/)\n",
    "* [Hyperparameter tuning a Tensorflow model on Kubeflow with GPU cluster](https://github.com/loomlike/hyperparameter-tuning-on-kubernetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "* Docker (if you want to create your own docker image) - To install, see [docker site](https://docs.docker.com/install/).\n",
    "* Azure CLI - The easiest way is to use [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/).\n",
    "  - You need the Azure CLI version 2.0.64 or later installed and configured. Run `az --version` to find the version. If you need to install or upgrade, see [Install Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest#update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Surprise version: 1.0.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import surprise\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "\n",
    "from reco_utils.common.constants import SEED\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_random_split\n",
    "from reco_utils.evaluation.python_evaluation import rmse, precision_at_k, ndcg_at_k\n",
    "from reco_utils.recommender.surprise.surprise_utils import compute_rating_predictions, compute_ranking_predictions\n",
    "from reco_utils.kubeflow.manifest.utils import (\n",
    "    choice,\n",
    "    uniform,\n",
    "    make_hypertune_manifest,\n",
    "    make_worker_spec,\n",
    ")\n",
    "from reco_utils.kubeflow.manifest import (\n",
    "    Goal,\n",
    "    SearchType,\n",
    "    WorkerType,\n",
    ")\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Surprise version: {}\".format(surprise.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "During the setup, we create AKS cluster and install Kubeflow on it.\n",
    "\n",
    "#### 1.1 AKS setup\n",
    "To create AKS and cluster, first make sure you signed in to use Azure CLI with a correct subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the subscription, run `az account set --subscription <YOUR-SUBSCRIPTION-NAME-OR-ID>`.\n",
    "\n",
    "Next, **set** desired *resource group name* and *AKS name* as well as the *region* you want to create the resources at to the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RG_NAME = \"reco-aks-rg\"  # YOUR-RESOURCE-GROUP-NAME\n",
    "AKS_NAME = \"reco-aks\"    # RESOURCE-NAME\n",
    "LOCATION = \"eastus\"      # RESOURCE-REGION. To get all the available region, run 'az account list-locations' and see 'name' key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following commands to create the resources. This example will create **eight** [Standard_D2_v2](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-general#dv2-series) CPU VM nodes for the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourceGroups/reco-aks-rg\",\n",
      "  \"location\": \"eastus\",\n",
      "  \"managedBy\": null,\n",
      "  \"name\": \"reco-aks-rg\",\n",
      "  \"properties\": {\n",
      "    \"provisioningState\": \"Succeeded\"\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": null\n",
      "}\n",
      "\u001b[K{- Finished ..ion done[############################################]  100.0000%\n",
      "  \"aadProfile\": null,\n",
      "  \"addonProfiles\": {\n",
      "    \"omsagent\": {\n",
      "      \"config\": {\n",
      "        \"logAnalyticsWorkspaceResourceID\": \"/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourcegroups/defaultresourcegroup-eus/providers/microsoft.operationalinsights/workspaces/defaultworkspace-03909a66-bef8-4d52-8e9a-a346604e0902-eus\"\n",
      "      },\n",
      "      \"enabled\": true\n",
      "    }\n",
      "  },\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"availabilityZones\": null,\n",
      "      \"count\": 8,\n",
      "      \"enableAutoScaling\": null,\n",
      "      \"maxCount\": null,\n",
      "      \"maxPods\": 110,\n",
      "      \"minCount\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"orchestratorVersion\": \"1.12.8\",\n",
      "      \"osDiskSizeGb\": 100,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"provisioningState\": \"Succeeded\",\n",
      "      \"type\": \"AvailabilitySet\",\n",
      "      \"vmSize\": \"Standard_D2_v2\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"apiServerAuthorizedIpRanges\": null,\n",
      "  \"dnsPrefix\": \"reco-aks-reco-aks-rg-03909a\",\n",
      "  \"enablePodSecurityPolicy\": null,\n",
      "  \"enableRbac\": true,\n",
      "  \"fqdn\": \"reco-aks-reco-aks-rg-03909a-c4c61b55.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/03909a66-bef8-4d52-8e9a-a346604e0902/resourcegroups/reco-aks-rg/providers/Microsoft.ContainerService/managedClusters/reco-aks\",\n",
      "  \"kubernetesVersion\": \"1.12.8\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCrMEhu8Ve8mbQxrxFUoJ8qusCZJsxyxbIXAjfkFnpZUkhR4cfbJ+x2rjoRUbnikLLgOBZ1d6E58Q5fp3oHRklnG8U05Z4/Y5nnoKpbVPo2BUFruZcEpP/GeEqRosZm5UZ6tMjielEYt/W4uEtPqrVClGlh61MdFL/y74CwikR7yD978s3SgIB5QV/6s7zNn4rjCXh3iIFIVY1TjzbIAizWYzVTm2WzE98HWs5BWcdtYCDVrIJ45ueS6PkCCV6NgDM2MOyVS92YXhjxNJU/lihqeARIN75XmvIDd4CuDV5gjpA7scQycWMF/XUyD6d/FqhNXB3UpW8Toa+G9Nf8oqQf jun@Juns-MacBook-Pro.local\\n\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"reco-aks\",\n",
      "  \"networkProfile\": {\n",
      "    \"dnsServiceIp\": \"10.0.0.10\",\n",
      "    \"dockerBridgeCidr\": \"172.17.0.1/16\",\n",
      "    \"networkPlugin\": \"kubenet\",\n",
      "    \"networkPolicy\": null,\n",
      "    \"podCidr\": \"10.244.0.0/16\",\n",
      "    \"serviceCidr\": \"10.0.0.0/16\"\n",
      "  },\n",
      "  \"nodeResourceGroup\": \"MC_reco-aks-rg_reco-aks_eastus\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"reco-aks-rg\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"24260096-fd77-4396-aa10-ff2ede624eaf\",\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create resource group\n",
    "!az group create --name {RG_NAME} --location {LOCATION}\n",
    "\n",
    "# Create AKS cluster\n",
    "!az aks create \\\n",
    "    --resource-group {RG_NAME} \\\n",
    "    --name {AKS_NAME} \\\n",
    "    --node-count 8 \\\n",
    "    --node-vm-size Standard_D2_v2 \\\n",
    "    --enable-addons monitoring \\\n",
    "    --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an AKS cluster may take few minutes. If the creation is successful, you'll see something like:\n",
    "```\n",
    "{- Finished ..ion done[############################################]  100.0000%\n",
    "  \"aadProfile\": null,\n",
    "    \"addonProfiles\": {\n",
    "      \"omsagent\": {\n",
    "        \"config\": {\n",
    "  ...\n",
    "```\n",
    "\n",
    "Now, install [Kubernetes CLI](https://docs.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-cluster#install-the-kubernetes-cli) `kubectl` for running commands against the cluster. If you already installed kubectl, skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az aks install-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the CLI to your cluster by runing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"reco-aks\" as current context in /Users/jun/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials --resource-group {RG_NAME} --name {AKS_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error, check if you have read/write permissions on the kubernetes config file. In a linux machine, the file will be at `~/.kube/config`.\n",
    "\n",
    "To verify the connection of CLI to your cluster, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS   ROLES   AGE   VERSION\r\n",
      "aks-nodepool1-19966255-0   Ready    agent   59m   v1.12.8\r\n",
      "aks-nodepool1-19966255-1   Ready    agent   59m   v1.12.8\r\n",
      "aks-nodepool1-19966255-2   Ready    agent   59m   v1.12.8\r\n",
      "aks-nodepool1-19966255-3   Ready    agent   59m   v1.12.8\r\n",
      "aks-nodepool1-19966255-4   Ready    agent   58m   v1.12.8\r\n",
      "aks-nodepool1-19966255-5   Ready    agent   59m   v1.12.8\r\n",
      "aks-nodepool1-19966255-6   Ready    agent   58m   v1.12.8\r\n",
      "aks-nodepool1-19966255-7   Ready    agent   58m   v1.12.8\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the connection is successful, the nodes information will be printed out like:\n",
    "```\n",
    "NAME                       STATUS    ROLES     AGE       VERSION\n",
    "aks-nodepool1-17965807-0   Ready     agent     11m       v1.12.8\n",
    "...\n",
    "```\n",
    "\n",
    "#### 1.2 Kubeflow setup\n",
    "Kubeflow makes use of *[ksonnet](https://www.kubeflow.org/docs/components/ksonnet/)* to help manage deployments.\n",
    "\n",
    "First, setup environment variables and download the ksonnet file by running the following scripts.\n",
    "Please note that here we use **ubuntu** version of *ksonnet* and Kubeflow deployment tool, *kfctl* (**not kubectl**).\n",
    "\n",
    "If you use a different OS, please refer each application's official document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OS_TYPE\"] = \"linux\"  # Use \"darwin\" for MacOS or darwin linux\n",
    "os.environ[\"KS_VER\"] = \"0.13.1\"\n",
    "os.environ[\"KS_PKG\"] = \"ks_{0}_{1}_amd64\".format(os.environ[\"KS_VER\"], os.environ[\"OS_TYPE\"])\n",
    "os.environ[\"PATH\"] = \"{0}:{1}/bin/{2}\".format(\n",
    "    os.environ[\"PATH\"],\n",
    "    os.environ[\"HOME\"],\n",
    "    os.environ[\"KS_PKG\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ks_0.13.1_darwin_amd64/CHANGELOG.md\n",
      "x ks_0.13.1_darwin_amd64/CODE-OF-CONDUCT.md\n",
      "x ks_0.13.1_darwin_amd64/CONTRIBUTING.md\n",
      "x ks_0.13.1_darwin_amd64/LICENSE\n",
      "x ks_0.13.1_darwin_amd64/README.md\n",
      "x ks_0.13.1_darwin_amd64/ks\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O /tmp/${KS_PKG}.tar.gz https://github.com/ksonnet/ksonnet/releases/download/v${KS_VER}/${KS_PKG}.tar.gz -q\n",
    "mkdir -p ${HOME}/bin\n",
    "tar -xvf /tmp/${KS_PKG}.tar.gz -C ${HOME}/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download, extract Kubeflow and deploy it. For more details about this process, please see [installation instruction of Kubeflow on Azure](https://www.kubeflow.org/docs/azure/deploy/install-kubeflow/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KFCTL_VER\"] = \"0.5.1\"\n",
    "os.environ[\"KFCTL_PKG\"] = \"kfctl_v{}_{}\".format(os.environ[\"KFCTL_VER\"], os.environ[\"OS_TYPE\"])\n",
    "os.environ[\"PATH\"] = \"{0}:{1}/bin\".format(\n",
    "    os.environ[\"PATH\"],\n",
    "    os.environ[\"HOME\"]\n",
    ")\n",
    "os.environ['KFAPP'] = \"kfapp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x ./kfctl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget -O /tmp/${KFCTL_PKG}.tar.gz https://github.com/kubeflow/kubeflow/releases/download/v${KFCTL_VER}/${KFCTL_PKG}.tar.gz -q\n",
    "tar -xvf /tmp/${KFCTL_PKG}.tar.gz -C ${HOME}/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2019-06-16T15:16:36-04:00\" level=info msg=\"deploying kubeflow application\" filename=\"cmd/apply.go:35\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kfctl init ${KFAPP}\n",
    "cd ${KFAPP}\n",
    "kfctl generate k8s\n",
    "kfctl apply k8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the deployment, check kubeflow pods as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                       READY   STATUS              RESTARTS   AGE\r\n",
      "ambassador-7b8477f667-bnrzk                                1/1     Running             0          2m7s\r\n",
      "ambassador-7b8477f667-mwkvz                                1/1     Running             0          2m7s\r\n",
      "ambassador-7b8477f667-w7m7j                                1/1     Running             0          2m7s\r\n",
      "argo-ui-9cbd45fdf-9mmfx                                    1/1     Running             0          119s\r\n",
      "centraldashboard-796c755dcf-zxhcm                          1/1     Running             0          111s\r\n",
      "jupyter-web-app-589f8756c9-pb5s5                           1/1     Running             0          104s\r\n",
      "katib-ui-7c6997fd96-q6wdb                                  1/1     Running             0          90s\r\n",
      "metacontroller-0                                           1/1     Running             0          81s\r\n",
      "minio-594df758b9-58dfp                                     1/1     Running             0          66s\r\n",
      "ml-pipeline-75b5d4585-4hjcs                                1/1     Running             0          64s\r\n",
      "ml-pipeline-persistenceagent-7ffd468c4b-5dhxd              1/1     Running             0          62s\r\n",
      "ml-pipeline-scheduledworkflow-56c8f5bc9b-2clq5             1/1     Running             0          63s\r\n",
      "ml-pipeline-ui-858f7f979d-8s9q4                            1/1     Running             0          61s\r\n",
      "ml-pipeline-viewer-controller-deployment-cc7fb8dfd-j7pgd   1/1     Running             0          62s\r\n",
      "mysql-5d5b5475c4-lr4tp                                     0/1     ContainerCreating   0          65s\r\n",
      "notebooks-controller-685db44f8c-56g57                      1/1     Running             0          74s\r\n",
      "pytorch-operator-9996bcb49-hqkw8                           1/1     Running             0          53s\r\n",
      "studyjob-controller-57cb6746ff-dqrg2                       1/1     Running             0          89s\r\n",
      "tensorboard-76dffc9ffc-hmmcx                               0/1     ContainerCreating   0          47s\r\n",
      "tf-job-dashboard-84bdddd5cc-t4h4x                          1/1     Running             0          39s\r\n",
      "tf-job-operator-8486555578-x77bj                           1/1     Running             0          39s\r\n",
      "vizier-core-bcc86677d-ckx7n                                0/1     Running             2          89s\r\n",
      "vizier-core-rest-68c7577f84-xj47k                          1/1     Running             0          90s\r\n",
      "vizier-db-54f46c46c6-8sqk8                                 0/1     ContainerCreating   0          89s\r\n",
      "vizier-suggestion-bayesianoptimization-97f4f76dd-bbkhp     1/1     Running             0          91s\r\n",
      "vizier-suggestion-grid-6f94f98f9d-flbkb                    1/1     Running             0          90s\r\n",
      "vizier-suggestion-hyperband-68f4cc7f5d-rsf9k               1/1     Running             0          90s\r\n",
      "vizier-suggestion-random-6ff5b8f6d8-58nmh                  1/1     Running             0          89s\r\n",
      "workflow-controller-d5cb6468d-762qk                        1/1     Running             0          119s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the namespace to be \"kubeflow\" so that we don't need to use `-n kubeflow` argument for every *kubectl* command in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"reco-aks\" modified.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context {AKS_NAME} --namespace=kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Persistent volumn setup\n",
    "One last thing we should do before moving to the next step is to create a persistent volumn to store our dataset. A PersistentVolumeClaim (PVC) is a request for storage by a user. For details, see [persistent volumes with Azure files](https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv). Here, we create **100G** size storage, which is defined in *[reco_utils/kubeflow/manifest/azure-file-pvc.yaml](../../reco_utils/kubeflow/manifest/azure-file-pvc.yaml)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storageclass.storage.k8s.io/azurefile created\n",
      "clusterrole.rbac.authorization.k8s.io/system:azure-cloud-provider created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/system:azure-cloud-provider created\n",
      "persistentvolumeclaim/azurefile created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../../reco_utils/kubeflow/manifest/azure-file-sc.yaml\n",
    "!kubectl apply -f ../../reco_utils/kubeflow/manifest/azure-pvc-roles.yaml\n",
    "!kubectl apply -f ../../reco_utils/kubeflow/manifest/azure-file-pvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='check-pvc'></a>\n",
    "To verify the deployment, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\r\n",
      "azurefile   Pending                                      azurefile      4m30s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc azurefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Preparation\n",
    "#### 2.1 Dataset\n",
    "1. Download data and split into training, validation and testing sets\n",
    "2. Upload the training and validation sets to our PVC. To do that,\n",
    "  1. Attach a pod to the PVC\n",
    "  2. Copy the datasets onto the pod\n",
    "  3. Delete the pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "TRAIN_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_train.pkl\"\n",
    "VAL_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_val.pkl\"\n",
    "TEST_FILE_NAME = \"movielens_\" + MOVIELENS_DATA_SIZE + \"_test.pkl\"\n",
    "\n",
    "USERCOL = 'userID'\n",
    "ITEMCOL = 'itemID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.81k/4.81k [00:05<00:00, 921KB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0     196     242     3.0\n",
       "1     186     302     3.0\n",
       "2      22     377     1.0\n",
       "3     244      51     2.0\n",
       "4     166     346     1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[USERCOL, ITEMCOL, \"rating\"]\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = python_random_split(data, [0.7, 0.15, 0.15], seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "train_pickle_path = os.path.join(tmpdir.name, TRAIN_FILE_NAME)\n",
    "train.to_pickle(train_pickle_path)\n",
    "\n",
    "val_pickle_path = os.path.join(tmpdir.name, VAL_FILE_NAME)\n",
    "validation.to_pickle(val_pickle_path)\n",
    "\n",
    "test_pickle_path = os.path.join(tmpdir.name, TEST_FILE_NAME)\n",
    "test.to_pickle(test_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before move forward, make sure your PVC has been deployed. Check the deployment status by re-running the command we executed earlier: `!kubectl get pvc azurefile` or go to the [earlier cell](#check-pvc) we checked the PVC status and re-run it.\n",
    "\n",
    "Once the PVC  we create a pod by using [reco_utils/kubeflow/manifest/pvc-loader.yaml](../../reco_utils/kubeflow/manifest/pvc-loader.yaml) to upload the datasets into `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete pod pvc-loader  # Delete if the pod already exists\n",
    "!kubectl apply -f ../../reco_utils/kubeflow/manifest/pvc-loader.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data files\n",
    "!kubectl cp {train_pickle_path} pvc-loader:/data/\n",
    "!kubectl cp {val_pickle_path} pvc-loader:/data/\n",
    "!kubectl cp {test_pickle_path} pvc-loader:/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "!kubectl exec pvc-loader -- bash -c \"ls /data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the data, we don't need the pod anymore, so remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete pod pvc-loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Training scripts\n",
    "\n",
    "We prepare a training script [reco_utils/kubeflow/svd_training.py](../../reco_utils/kubeflow/svd_training.py) for the hyperparameter tuning, which will log our target metrics such as [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) and/or [NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) to *Katib* so that we can track the metrics and optimize the primary metric. At the end, the script also saves the trained model to the output folder so that we can download and validate the model on test set later.\n",
    "\n",
    "We use the Docker image containing our Recommender repo as well as the training script. For more details, see [reco_utils/kubeflow/docker/Dockerfile](../../reco_utils/kubeflow/docker/Dockerfile).\n",
    "\n",
    "#### 2.3 Parameters\n",
    "\n",
    "We define a search space for the hyperparameters. All the parameter values will be passed into our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"movielens-\" + MOVIELENS_DATA_SIZE + \"-svd\"\n",
    "PRIMARY_METRIC = 'precision_at_k'\n",
    "PRIMARY_METRIC_GOAL = Goal.MAXIMIZE\n",
    "IDEAL_METRIC_VALUE = 1.0\n",
    "RATING_METRICS = ['rmse']\n",
    "RANKING_METRICS = ['precision_at_k', 'ndcg_at_k']  \n",
    "\n",
    "REMOVE_SEEN = True\n",
    "K = 10\n",
    "RANDOM_STATE = 0\n",
    "VERBOSE = True\n",
    "NUM_EPOCHS = 30\n",
    "BIASED = True\n",
    "\n",
    "MAX_TOTAL_RUNS = 100  # Number of runs (training-and-evaluation) to search for the best hyperparameters. \n",
    "MAX_CONCURRENT_RUNS = 8\n",
    "\n",
    "# PVC mount path\n",
    "STORAGE_MOUNT_PATH = \"/data\"\n",
    "\n",
    "script_params = {\n",
    "    '--datastore': STORAGE_MOUNT_PATH,\n",
    "    '--train-datapath': TRAIN_FILE_NAME,\n",
    "    '--validation-datapath': VAL_FILE_NAME,\n",
    "    '--surprise-reader': \"ml-100k\",\n",
    "    '--rating-metrics': RATING_METRICS,\n",
    "    '--ranking-metrics': RANKING_METRICS,\n",
    "    '--usercol': USERCOL,\n",
    "    '--itemcol': ITEMCOL,\n",
    "    '--k': K,\n",
    "    '--random-state': RANDOM_STATE,\n",
    "    '--epochs': NUM_EPOCHS,\n",
    "}\n",
    "\n",
    "if BIASED:\n",
    "    script_params['--biased'] = ''\n",
    "if VERBOSE:\n",
    "    script_params['--verbose'] = ''\n",
    "if REMOVE_SEEN:\n",
    "    script_params['--remove-seen'] = ''\n",
    "\n",
    "# hyperparameters search space\n",
    "# We do not set 'lr_all' and 'reg_all' because they will be overwritten by the other lr_ and reg_ parameters\n",
    "hyperparams = {\n",
    "    '--n-factors': choice([10, 50, 100, 150, 200]),\n",
    "    '--init-mean': uniform(-0.5, 0.5),\n",
    "    '--init-std-dev': uniform(0.01, 0.2),\n",
    "    '--lr-bu': uniform(1e-6, 0.1), \n",
    "    '--lr-bi': uniform(1e-6, 0.1), \n",
    "    '--lr-pu': uniform(1e-6, 0.1), \n",
    "    '--lr-qi': uniform(1e-6, 0.1), \n",
    "    '--reg-bu': uniform(1e-6, 1),\n",
    "    '--reg-bi': uniform(1e-6, 1), \n",
    "    '--reg-pu': uniform(1e-6, 1), \n",
    "    '--reg-qi': uniform(1e-6, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create worker and study manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to repeat the experiment without over-writing the previous StudyJob deployment.\n",
    "TAG = \"random-1\"\n",
    "# Change this to select different search algorithm\n",
    "SEARCH_TYPE = SearchType.RANDOM\n",
    "\n",
    "worker_spec = make_worker_spec(\n",
    "    name=EXP_NAME,\n",
    "    tag=TAG,\n",
    "    worker_type=WorkerType.WORKER,\n",
    "    image_name='loomlike/reco',\n",
    "    entry_script='/app/reco_utils/kubeflow/svd_training.py',\n",
    "    params=script_params,\n",
    "    is_hypertune=True,\n",
    "    storage_path=STORAGE_MOUNT_PATH,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "studyjob_name, studyjob_file = make_hypertune_manifest(\n",
    "    search_type=SEARCH_TYPE,\n",
    "    total_runs=MAX_TOTAL_RUNS,\n",
    "    concurrent_runs=MAX_CONCURRENT_RUNS,\n",
    "    primary_metric=PRIMARY_METRIC,\n",
    "    goal=PRIMARY_METRIC_GOAL,\n",
    "    ideal_metric_value=IDEAL_METRIC_VALUE,\n",
    "    metrics=RATING_METRICS+RANKING_METRICS,\n",
    "    hyperparams=hyperparams,\n",
    "    worker_spec=worker_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments\n",
    "Now, we deploy the studyjob and monitor the status by using *kubectl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Delete previous StudyJob of the same name if exists\n",
    "!kubectl delete studyjob {studyjob_name}\n",
    "\n",
    "# Create a StudyJob\n",
    "!kubectl create -f {studyjob_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe studyjob {studyjob_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check more details about each **job** (trial) and its **pod**, you can use **kubectl** commands like:\n",
    "```\n",
    "# To list all the jobs\n",
    "kubectl get job \n",
    "\n",
    "# To check a specific job\n",
    "kubectl describe job <your-job-id>\n",
    "\n",
    "# To check a specific pod\n",
    "kubectl describe pod <your-pod-id>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Dashboard\n",
    "To access Kubeflow dashboard, we use port-tunneling to access *ambassador* service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x1a25629320>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.Popen(\"kubectl port-forward svc/ambassador 8080:80\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, open [localhost:8080](http://localhost:8080) from a browser, go to **Katib Dashboard** tab and select the StudyJob you want to see from the list.\n",
    "\n",
    "You also can create a new job from the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Screen-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Result query\n",
    "\n",
    "You can also query the results by using REST API or gRPC. You can find gRPC API from [Katib official github repo](https://github.com/kubeflow/katib/blob/master/pkg/api/v1alpha1/api.proto).\n",
    "\n",
    "Here, we use our helper functions to query the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Test\n",
    "We got the best-performing hyperparameter. Now, we evaluate the metrics on the test data. To do that, download the best model we stored while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = surprise.dump.load('aml_model/model.dump')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "predictions = compute_rating_predictions(svd, test, usercol=USERCOL, itemcol=ITEMCOL)\n",
    "for metric in RATING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, predictions)\n",
    "\n",
    "all_predictions = compute_ranking_predictions(svd, train, usercol=USERCOL, itemcol=ITEMCOL, recommend_seen=RECOMMEND_SEEN)\n",
    "for metric in RANKING_METRICS:\n",
    "    test_results[metric] = eval(metric)(test, all_predictions, col_prediction='prediction', k=K)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Concluding Remarks\n",
    "\n",
    "We showed how to tune **all** the hyperparameters accepted by Surprise SVD simultaneously, by utilizing Kubeflow on AKS.\n",
    "\n",
    "TODO add insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup\n",
    "\n",
    "To uninstall Kubeflow,\n",
    "```\n",
    "cd ${KF_APP}\n",
    "# If you want to delete all the resources, including storage.\n",
    "kfctl delete all --delete_storage\n",
    "# If you want to preserve storage, which contains metadata and information\n",
    "# from mlpipeline.\n",
    "kfctl delete all\n",
    "```\n",
    "\n",
    "To remove AKS cluster,\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco_base",
   "language": "python",
   "name": "reco_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
