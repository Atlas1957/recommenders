\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage[utf8]{inputenc}
\usepackage{adjustbox}
\usepackage[flushleft]{threeparttable}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{meila00a}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Microsoft Recommenders}{Meila and Jordan}
\firstpageno{1}

\begin{document}

\title{Microsoft Recommenders: best practices for building industry-grade recommender system}

\author{\name Andreas Argyriou \email anargyri@microsoft.com \\
  \name Miguel Gonzalez-Fierro \email migonza@microsoft.com \\
  \name Scott Graham \email scgraham@microsoft.com \\
  \name Nikhil Joglekar \email nikhilj@microsoft.com \\
  \name Jun Ki Min \email jumin@microsoft.com \\
  \name Jeremy Reynolds \email jeremr@microsoft.com \\
  \name Tao Wu \email wutao@microsoft.com \\
  \name Le Zhang \email zhle@microsoft.com \\
  \addr Microsoft \\
  One Microsoft Way, USA
}

\editor{editors here}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  This paper talks about the design philosophy and implementation methodology of Microsoft Recommenders, which is aimed at providing researchers and developers with best-practices for building  industry-grade recommendation system at scale.
\end{abstract}

\begin{keywords}
  artificial intelligence, recommender system, software engineering
\end{keywords}

\section{Introduction}
The open source GitHub repository, \verb|Microsoft Recommenders| was created for sharing the best practices in building industry standard recommender system for varieties of application scenarios. The repository aims at helping developers, scientists and researchers to quickly build production-ready recommendation systems as well as to prototype novel ideas using the provided utility functions.

In this paper the \verb|Microsoft Recommenders| repository is introduced with its design principles, technologies under the hood, and technical advantage over the existing approaches.

\section{Microsoft Recommenders}
The following sub-sections brief the design thinking of \verb|Microsoft Recommenders|.

\subsection{Principles}
\begin{enumerate}
\item \emph{The repository covers a wide spectrum of recommender algorithms. It includes classic methods such as collaborative filtering and factorization machine, as well as more recent deep learning algorithms with enhanced model accuracy, explainability, and scalability.} The wide breadth of recommendation algorithms, as well as the DevOps pipeline that serves as a backbone underneath the codebase, offers convenience to the researchers and developers for trying different algorithms and selecting the optimal one for particular use case. 
\item \emph{The algorithms are implemented and optimized for easy adoption and customization.} It is a common yet challenging task to generalize implementations of various components in a recommender system for development efficiency. The repository provides modular and reusable assets as utility functions (e.g. temporal or stratified split between training and testing data) with consistent formats, coding style, and standard APIs, to enable ready-for-use scenarios as well as user-customized solutions in a recommender system. In addition, the theoretical discussion and code implementation are put into Jupyter notebooks to provide interactive experience for the developers and have them quickly understand, implement or customize different recommendation algorithms. When necessary, the notebooks can be conveniently converted to production-ready codes with minimal efforts on refactoring. 
\item \emph{The Recommenders repository avails heterogeneous computing platforms and data storage media on cloud that meet the requirements of the different workloads in a recommender system pipeline.} A commonly seen problem that arises in recommender system design is that the backend data processing pipeline and model building/serving pipeline should be designed and implemented under certain set of engineering constraints to satisfy the requirements of the frontend applications. To harness the complexity in system design, the Recommenders repository provides reference architectures that demonstrates how to build an enterprise level end-to-end recommendation system. For example, the reference architecture for a real-time recommender system shows data ingestion, data storage, model building, model deployment and model consumption by using various technologies like Spark, distributed database and Kubernetes on the cloud. 
\end{enumerate}

\subsection{Design of the repository}
The major deliverables in the repository are \verb|notebooks| and \verb|reco_utils|. Both of implemented in \verb|Python| programming language, which is the most popular programming tool to the contemporary data science and machine learning problems.

\begin{itemize}
 \item \verb|notebooks|
 \item \verb|reco_utils| are reusable assets for common tasks in building recommender system. Such tasks include data preparation, evaluation, etc. To favor such operations with scalability consideration, some of the \verb|reco_utils| are implemented in both Python and PySpark versions.
\end{itemize}

\subsection{Technologies}
\verb|Microsoft Recommenders| supports various platforms for building recommender systems with the cutting-edge recommendation algorithms.

\subsubsection{Recommender algorithm and auxiliary operations}
 The table below summarizes the recommender algorithms collected in the \verb|Microsoft Recommenders| thus far \citep{ke2017lightgbm,wang2018dkn,lian2018xdeepfm,howard2018fastai,he2017neural,salakhutdinov2007restricted,cheng2016wide,diev2015sar,koren2009matrix} The table also shows the technologies used in the implementation and use case scenario of these algorithms.

\begin{adjustbox}{angle=90}
  \footnotesize
  \begin{threeparttable}
  \caption{List of the collected recommender algorithms and their supported technology platforms}
    \begin{tabular}{|p{8cm}|p{3.5cm}|p{3.5cm}|p{6cm}|}
    \hline
    \textbf{Algorithms} & \textbf{Environment} & \textbf{Type} & \textbf{Description} \\
    \hline
    Alternating Least Square & Spark cluster & Collaborative filtering & Matrix factorization algorithm \\
    \hline
    LightGBM/Gradient Boosting Tree & CPU / Spark cluster & Content-based filtering & Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems \\
    \hline
    Deep knowledge-aware network\textsuperscript{*} & CPU / GPU & Content-based filtering & Deep learning algorithm with knowledge graph enhancement \\
    \hline
    Extreme Deep Factorization Machine (xDeepFM)\textsuperscript{*} & CPU / GPU & Hybrid & Deep learning based algorithm for implicit and explicit feedback with user/item features \\
    \hline
    FastAI Embedding Dot Bias (FAST) & CPU / GPU & Collaborative filtering & General purpose algorithm with embeddings and biases for users and items \\
    \hline
    Neural Collaborative filtering (NCF)\textsuperscript{*} & CPU / GPU & Collaborative filtering & Deep learning algorithm with enhanced performance for implicit feedback \\
    \hline
    Restricted Boltzmann Machines (RBM)\textsuperscript{*} & CPU / GPU & Collaborative filtering & Neural network based algorithm for learning the underlying probability distribution for explicit or implicit feedback \\
    \hline
    Wide and Deep & CPU / GPU & Hybrid & Deep learning algorithm that can memorize feature interactions and generalize user features \\
    \hline
    Riemannian Low-rank Matrix Completion (RLRMC)\textsuperscript{*} & CPU & Collaborative filtering & Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption. \\
    \hline
    Simple Algorithm for Recommendation (SAR)\textsuperscript{*} & CPU & Collaborative filtering & Similarity-based algorithm for implicit feedback dataset \\
    \hline
    Surprise/Singular Value Decomposition (SVD) & CPU & Collaborative filtering & Matrix factorization algorithm for predicting explicit rating feedback in datasets that are not very large \\
    \hline
    Vowpal Wabbit Family (VW) & CPU (online training) & Content-based filtering & Fast online learning algorithms, great for scenarios where user features / context are constantly changing \\
    \hline
    \end{tabular}
    \begin{tablenotes}
      \scriptsize
      \item Recommender algorithms labelled with "*" indicate that they are implemented natively in the repository.
    \end{tablenotes}
  \end{threeparttable}
\end{adjustbox}

\subsubsection{Model development and optimization}
The \verb|Microsoft Recommender| repository offers reusable assets for the whole life cycle of developing a recommender system, which include data preparation, model development, model evaluation, and model evaluation. For example, the data preparation utilities make the  

Hyperparameter tuning plays a vital part in model selection. Intelligent methods for tuning hyper parameters for recommender models, especially those built with deep learning algorithms, are highly desirable. The repository provides the best practices for optimizing model performance with the state-of-the-arts tuning methods like Bayesian optimization, Hyperband, etc.

\subsubsection{System production}
To avail building industry-grade deployable recommender system, the repository demonstrated reference architecture of building scalable recommender system \footnote{The reference architecture is demonstrated on Microsoft Azure cloud platform.}

\section{Comparison}

\section{Conclusion}

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

% Acknowledgements should go at the end, before appendices and references

\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

\vskip 0.2in
\bibliography{references}

\end{document}
