\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage[utf8]{inputenc}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{meila00a}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Meila and Jordan}
\firstpageno{1}

\begin{document}

\title{Microsoft/Recommenders: Best practices for building industry-grade recommender system}

\author{\name Le Zhang \email your@email.com \\
       \addr One Microsoft Way \\
       Redmond, WA 98052, USA
       \AND
       \name Jun-Ki Min \email jun.min@microsoft.com \\
       \addr 1 Memorial Dr \\
       Cambridge, MA 02142, USA
       \AND
       \name Authors \email your email \\
       \addr your address line 1 \\
       \addr your address line 2 \\
       \addr your address line 3}

\editor{editors here}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  This paper talks about the design philosophy and implementation methodology of Microsoft/Recommenders, which is aimed at providing researchers and developers with best-practices for building  industry-grade recommendation system at scale.
\end{abstract}

\begin{keywords}
  artificial intelligence, recommender system, software engineering
\end{keywords}

\section{Introduction}
Recent decades have witnessed a great proliferation of recommendation systems. The technology has brought significant profits to many business verticals. From earlier algorithms such as similarity based collaborative filtering to the latest deep neural network based methods, recommendation technologies have evolved dramatically, which, to some extent, makes it challenging to practitioners to select and customize the optimal algorithms for a specific business scenario. In addition, operations such as data preprocessing, model evaluation, system operationalization etc. play a significant role in the lifecycle of developing a recommendation system; however, they are often neglected by practitioners. 
Based on extensive experience in productization of recommendation systems in a variety of real-world application domains, an open source GitHub repository, \texttt Microsoft/Recommenders was created for sharing the best practices in building industry standard recommender system for varieties of application scenarios. The repository aims at helping developers, scientists and researchers to quickly build production-ready recommendation systems as well as to prototype novel ideas using the provided utility functions.

In this paper we introduce Microsoft/Recommenders by describing the general principles, models and topics covered by the repository which includes implementation of modern state-of-the-art recommendation algorithms as well as more practical topics such as hyper-parameter tuning and operationalization.

\section{Microsoft/Recommenders}
\subsection{General principles of the repository}
\begin{enumerate}
\item \emph{The repository covers a wide spectrum of recommender algorithms. It includes classic methods such as collaborative filtering and factorization machine, as well as more recent deep learning algorithms with enhanced model accuracy, interpretability, and scalability.} The wide breadth of recommendation algorithms, as well as the DevOps pipeline that serves as a backbone underneath the codebase, offers convenience to the researchers and developers for trying different algorithms and selecting the optimal one for particular use case. 
\item \emph{The algorithms are implemented and optimized for easy adoption and customization.} It is a common yet challenging task to generalize implementations of various components in a recommender system for development efficiency. The repository provides modular and reusable assets as utility functions (e.g. temporal or stratified split between training and testing data) with consistent formats, coding style, and standard APIs, to enable ready-for-use scenarios as well as user-customized solutions in a recommender system. In addition, the theoretical discussion and code implementation are put into Jupyter notebooks to provide interactive experience for the developers and have them quickly understand, implement or customize different recommendation algorithms. When necessary, the notebooks can be conveniently converted to production-ready codes with minimal efforts on refactoring. 
\item \emph{The Recommenders repository avails heterogeneous computing platforms and data storage media on cloud that meet the requirements of the different workloads in a recommender system pipeline.} A commonly seen problem that arises in recommender system design is that the backend data processing pipeline and model building/serving pipeline should be architected under certain set of engineering constraints to satisfy the requirements of the frontend applications. To harness the complexity in system design, the Recommenders repository provides reference architectures that demonstrates how to build an enterprise level end-to-end recommendation system. For example, the reference architecture for a real-time recommender system shows data ingestion, data storage, model building, model deployment and model consumption by using various technologies like Spark, distributed database and Kubernetes on the cloud. 
\item \emph{The repository provides an open, transparent, and collaborative platform for academic and open-source community for contributing and testing their own algorithms.} A major design principle of the repository is openness, to facilitate collaborations among researchers and developers in the field. More than 20 people contributed to the repository [4]. This is made available by the reusable and extensible utility functions. The recommenders repository became a well-recognized resource to acquire knowledge about recommendation engine beyond the basics. As of early May 2019, the repository has more than 3000 stars on Github and is forked close to 400 times. It was also covered and recognized by Hacker News, KDNuggets and Oâ€™Reilly Data Newsletter
\end{enumerate}


\subsection{Data preparation and model evaluation}
\subsubsection{Data preparation}
\subsubsection{Model evaluation}
\subsubsubsection{Offline evaluation and metrics}
\subsubsubsection{Online evaluation}
\begin{itemize}
  \item A/B testing
  \item Multi-armed bandit
\end{itemize}

\subsection{Modeling and optimization}
\subsubsection{Model deep dives}
We covers algorithm-level details about each recommender model so that users can select a model that fits to their problem.
\subsubsection{Hyper-parameter tuning}

\subsection{Operationalization}
\subsubsection{Large-scale model building and serving}
\subsubsubsection{Distributed computing platform and database}
\subsubsubsection{Containerization for real-time serving}

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

\section{Background}
% Some brief general introduction goes to here
This section lists recommender algorithms covered by Recommenders repository as of May 2019.

\subsection{Classic algorithms}
% Algorithms that have been practiced by developers and proven value in industry applications
\subsubsection{Collaborative filtering algorithms}
\begin{itemize}
  \item Memory-based algorithms
    \begin{itemize}
      \item Similarity-based models, Simple Algorithm for Recommendation (SAR), etc.
      \item Implementation challenges
    \end{itemize}
  \item Matrix factorization
    \begin{itemize} 
      \item singular value decomposition
      \item alternate leasting square
      \item nerual collaborative filtering
    \end{itemize}
\end{itemize}
\subsubsection{Content-based filtering algorithms}
\begin{itemize}
  \item logistic regression
  \item gradient boosting techniques
  \item factorization machine
\end{itemize}
\subsection{Emerging approaches}
\subsubsection{Deep learning applications}
\subsubsection{Enhancement with heterogeneous information}

% Acknowledgements should go at the end, before appendices and references

\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

\vskip 0.2in
\bibliography{sample}

\end{document}