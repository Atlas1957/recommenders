\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage[utf8]{inputenc}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{meila00a}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Meila and Jordan}
\firstpageno{1}

\begin{document}

\title{Microsoft Recommenders: Best practices for building industry-grade recommender system}

\author{\name Andreas Argyriou \email anargyri@microsoft.com \\
  \name Miguel Gonzalez-Fierro \email migonza@microsoft.com \\
  \name Scott Graham \email scgraham@microsoft.com \\
  \name Nikhil Joglekar \email nikhilj@microsoft.com \\
  \name Jun Ki Min \email jumin@microsoft.com \\
  \name Jeremy Reynolds \email jeremr@microsoft.com \\
  \name Tao Wu \email wutao@microsoft.com \\
  \name Le Zhang \email zhle@microsoft.com \\
  \addr Microsoft \\
  One Microsoft Way, USA
}

\editor{editors here}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  This paper talks about the design philosophy and implementation methodology of Microsoft Recommenders, which is aimed at providing researchers and developers with best-practices for building  industry-grade recommendation system at scale.
\end{abstract}

\begin{keywords}
  artificial intelligence, recommender system, software engineering
\end{keywords}

\section{Introduction}
The open source GitHub repository, \verb|Microsoft Recommenders|, was created for sharing the best practices in building industry standard recommender system for varieties of application scenarios. The repository aims at helping developers, scientists and researchers to quickly build production-ready recommendation systems as well as to prototype novel ideas using the provided utility functions.

In this paper we introduce Microsoft Recommenders by describing the general principles, models and topics covered by the repository which includes implementation of modern state-of-the-art recommendation algorithms as well as more practical topics such as hyper-parameter tuning and operationalisation. Microsoft Recommenders 

\section{Microsoft Recommenders}
\subsection{Principles}
\begin{enumerate}
\item \emph{The repository covers a wide spectrum of recommender algorithms. It includes classic methods such as collaborative filtering and factorization machine, as well as more recent deep learning algorithms with enhanced model accuracy, explainability, and scalability.} The wide breadth of recommendation algorithms, as well as the DevOps pipeline that serves as a backbone underneath the codebase, offers convenience to the researchers and developers for trying different algorithms and selecting the optimal one for particular use case. 
\item \emph{The algorithms are implemented and optimized for easy adoption and customization.} It is a common yet challenging task to generalize implementations of various components in a recommender system for development efficiency. The repository provides modular and reusable assets as utility functions (e.g. temporal or stratified split between training and testing data) with consistent formats, coding style, and standard APIs, to enable ready-for-use scenarios as well as user-customized solutions in a recommender system. In addition, the theoretical discussion and code implementation are put into Jupyter notebooks to provide interactive experience for the developers and have them quickly understand, implement or customize different recommendation algorithms. When necessary, the notebooks can be conveniently converted to production-ready codes with minimal efforts on refactoring. 
\item \emph{The Recommenders repository avails heterogeneous computing platforms and data storage media on cloud that meet the requirements of the different workloads in a recommender system pipeline.} A commonly seen problem that arises in recommender system design is that the backend data processing pipeline and model building/serving pipeline should be designed and implemented under certain set of engineering constraints to satisfy the requirements of the frontend applications. To harness the complexity in system design, the Recommenders repository provides reference architectures that demonstrates how to build an enterprise level end-to-end recommendation system. For example, the reference architecture for a real-time recommender system shows data ingestion, data storage, model building, model deployment and model consumption by using various technologies like Spark, distributed database and Kubernetes on the cloud. 
\end{enumerate}

\subsection{Technologies}
\subsubsection{Recommender algorithm}
\begin{table}[]
\begin{tabular}{lllll}
 & Algorithms  & Environment & Type & Description \\
 & Alternate Leasting Square & Spark & Collaborative filtering & Matrix factorization algorithm \\
 & Deep knowledge-aware network & Python CPU / Python GPU & Content-based filtering & Deep learning algorithm with knowledge graph enhancement  \\
\end{tabular}
\end{table}

\subsubsection{Model development and optimization}
\subsubsection{System production}

\subsection{Design of the package}
\verb|notebooks|
\verb|reco_utils| are reusable assets for common tasks in building recommender system. Such tasks include data preparation, evaluation, etc. To favor such operations with scalability consideration, some of the \verb|reco_utils| are implemented in both Python and PySpark versions.

\section{Comparison}

\section{Conclusion}

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

% Acknowledgements should go at the end, before appendices and references

\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

\vskip 0.2in
\bibliography{sample}

\end{document}
